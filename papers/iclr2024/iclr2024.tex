\documentclass{article}
\usepackage{iclr2024_conference}
\input{math_commands.tex}
\usepackage{hyperref}
\usepackage{url}
\usepackage{times}
\usepackage{graphicx}
\usepackage{array}
\usepackage{paralist}
\usepackage{float}
\usepackage[utf8]{inputenc}
\usepackage{threeparttable}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage{tabularx}
\usepackage{ragged2e}
\usepackage{adjustbox}
\usepackage{rotating}

\title{A Competition Winning Deep Reinforcement Learning Agent in microRTS}

\author{Scott Goodfriend \\
\texttt{goodfriend.scott@gmail.com} \\
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

% \iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\usepackage{iclr2024_anonymization}

\begin{document}

\maketitle
\begin{abstract}
Scripted agents have predominantly won the five
previous iterations of the IEEE microRTS ($\mu$RTS) competitions hosted at CIG and
CoG. Despite Deep Reinforcement Learning (DRL) algorithms making significant strides
in RTS-style games, their adoption has been limited in this primarily academic
competition due to the considerable training resources required and the complexity
inherent in creating and debugging such agents. \agentName\ is the first DRL agent
to win the IEEE microRTS competition. In a benchmark without performance
constraints, \agentName\ regularly defeated the two
prior competition winners. This first competition-winning DRL submission can be
a benchmark for future microRTS competitions and a starting point for future DRL
research. Iteratively fine-tuning the base policy and transfer learning to specific maps was 
critical to \agentName's winning performance, strategies that could be used in
economically training future DRL agents. Further work in Imitation Learning using Behavior Cloning and
fine-tuning such models with DRL has proven promising as an even more efficient way
to bootstrap models with novel behaviors.
\end{abstract}
\section{Introduction}
Deep reinforcement learning (DRL) has proven to be powerful at solving complex
problems requiring several steps to achieve a goal, such as Atari games \citep{DBLP:journals/corr/MnihKSGAWR13}, continuous
control tasks \citep{DBLP:journals/corr/LillicrapHPHETS15}, and even real-time strategy
(RTS) games like StarCraft II by AlphaStar \citep{Vinyals2019GrandmasterLI}. However, AlphaStar was trained with thousands of
CPUs and GPUs/TPUs for several weeks. RTS games are particularly challenging for DRL for
several reasons:
\begin{inparaenum}[(1)]
    \item the observation and action spaces are large and varied with different terrain and
        unit types;
    \item each unit type can have different actions and abilities;
    \item each action can control several units at once;
    \item rewards are sparse (win, loss, or tie) and delayed by possibly several
    thousand timesteps;
    \item winning requires combining tactical (micro) and strategic (macro) decisions;
    \item actions must be taken in real-time (i.e., the game won't wait for the agent to
        take an action);
    \item the agent might not have full visibility of the game state (i.e., fog of war); and
    \item events in the game might be non-deterministic.
\end{inparaenum}

microRTS (stylized as $\mu$RTS) is a minimalist, open-source, two-player, zero-sum RTS game testbed designed for research
purposes \citep{Ontan2013TheCM}. It includes many aspects of RTS games, simplified: different unit types, unit-specific
actions, terrain, resource collection and utilization to build units, and unit-to-unit combat
where units have different strengths and weaknesses. microRTS also supports fog of war
and non-determinism; however, these were disabled for the IEEE-CoG2023 microRTS
competition.

The IEEE microRTS competitions have been hosted at the Conference on Games (CoG) nearly
every year since 2019 and at the Conference on Computational Intelligence and Games
(CIG) before that since 2017 \citep{Ontañón_Barriga_Silva_Moraes_Lelis_2018}.
Competitors submit an agent that plays against other submissions and baselines in a round-robin tournament
on 12 different maps: 8 Open (known beforehand) and 4 Hidden (unknown until after the
competition results are released). Models in the IEEE microRTS competition are supposed
to submit actions every step within 100 ms. Without GPU acceleration, this is a significant constraint for deep neural
network agents.

This paper describes how the \agentName\ agent\footnote{\raiMicroRTSGitHubUrl} was
trained and became the first DRL agent to win a microRTS competition by winning at CoG
in 2023. The agent consists of 7 trained neural networks, taking 70
GPU-days to train. This significant training time combined with the general difficulty
in debugging and fine-tuning a DRL implementation could explain why DRL hasn't been
competitive so far. However, we demonstrate that transfer learning to specific maps was
critical to winning the competition and more economical than training from randomly
initialized weights. This strategy and the training framework can be a starting
point for future research and competition agents.

While microRTS doesn't support human players, the competitions have made several agents
available to use for imitation learning. Work following the
competition shows that behavior cloning and fine-tuning with DRL can be used to train a
competitive agent more economically. Using the same playthroughs to train the critic
heads on win-loss rewards means that DRL can be trained with just sparse win-loss
rewards, eliminating the handcrafted reward function required to train with DRL in microRTS.

\section{Related Work}
\subsection{MicroRTS-Py}
\citet{DBLP:journals/corr/abs-2105-13807} released MicroRTS-Py\footnote{\url{https://github.com/Farama-Foundation/MicroRTS-Py}}, an OpenAI Gym
wrapper for microRTS that includes a Proximal Policy Optimization (PPO, \citet{DBLP:journals/corr/SchulmanWDRK17}) implementation
trained on 1 of the Open maps (\texttt{16x16basesWorkers}). They added action composition, a shaped reward function, invalid action
masking, IMPALA-CNN (a convolutional neural network with residual blocks), and trained
against a diverse set of scripted agents. The agent achieved a
91\% win rate on a single map against a diverse set of competition bots.

In their ablation studies, they found invalid action masking was essential to have an agent that could compete at
the most basic level (82\% win rate with invalid action masking, 0\% without). Using the
residual block network IMPALA-CNN architecture instead of the Atari Nature CNN by
\citet{DBLP:journals/corr/MnihKSGAWR13} got the win rate up the rest of the way.

They experimented with two different ways to issue player actions: Unit Action Simulation (UAS)
and GridNet \citep{pmlr-v97-han19a}. UAS calls the policy iteratively on each unit, simulating the game state
after each unit action before issuing all unit actions combined to the game engine. GridNet
computes the actions for all units in a single policy call by computing unit action
logits for all grid positions and using a player action mask to ignore cells that don't
have any units owned by the player. UAS performed better than GridNet (91\% vs 89\%).
Despite UAS's better performance, MicroRTS-Py is deprecating UAS in favor of GridNet
because UAS's more complex implementation and difficulty to incorporate self-play and
imitation learning, both features important in \agentName\ and our further work.

They also tried self-play, training the policy in games where the policy played itself. 
They found self-play didn't improve the win rate,
neither when only using self-play nor when training with half self-play and half scripted
bots. The other policies were trained as only the first player while self-play required
the policy train as both players, so the comparison might not be equivalent.

We reimplement much of MicroRTS-Py and extend its capabilities to support training on
more maps, extend training capabilities, fix self-play, and add imitation learning.

\subsection{DeepMind AlphaStar}
\citet{Vinyals2019GrandmasterLI}'s AlphaStar is a grandmaster-level AI trained with DRL to play the RTS game
StarCraft II. They created an initial set of agents through imitation learning: supervised learning
using a dataset of observations, actions, and rewards to train a policy to mimic actions
from the dataset. The dataset was created by sampling replays by top-quartile human
players. The supervised agents were rated in the top 16\% of human players and used as
starting points for DRL. They created a league-based framework to train multiple agents
in parallel, each with different opponents to beat, thus creating a diverse set of
training agents. AlphaStar was trained on 3072 TPU cores and 50,400 preemptible CPU
cores for a duration of 44 days.

microRTS is a much simpler game than StarCraft II, both in game mechanics and simulation
cost. \citet{DBLP:journals/corr/abs-2105-13807} trained on a single map for 300 million
steps in less than 3 GPU-days. \agentName\ trained on 10 maps for 1.5 billion steps in
70 GPU-days. While \agentName\ didn't use supervised learning to bootstrap the agent,
our following work uses imitation learning to train a competitive agent.

AlphaStar's observation and action space is significantly different from microRTS. The
StarCraft II Learning Environment (PySC2) is made to be similar to a human player's
observations and controls. An AlphaStar action is 
\begin{inparaenum}[(1)]
    \item selecting an action type,
    \item selecting a subset of units to perform the action on, and
    \item selecting a target for the action (either a map location or visible unit).
\end{inparaenum}
Once supplied an action and a target, units will perform the action until the action is complete or the unit is interrupted.
microRTS requires the agent to give single step actions for each unit at each timestep.

\subsection{Lux AI Kaggle competitions}
The competition platform Kaggle hosts Simulation competitions where competitors
submit agents that play against other submitted agents in a turn-based game
environment. About once a year since 2020, Kaggle features an RTS-like Simulation competition:
Halite, Lux AI, Kore, and Lux AI Season 2. Rules-based agents won Halite, Kore, and Lux
AI Season 2. The first season Lux AI winning DRL agent by \citet{lux-ai-2021-winner} 
has many similarities to MicroRTS-Py: GridNet action space, reward shaping, and
actor-critic training algorithm (IMPALA with additional UPGO and TD($\lambda$) loss
terms, instead of PPO). Instead
of training with a shaped reward function through the entirety of training, \citet{lux-ai-2021-winner}
used shaped rewards on a smaller map before transitioning to sparse win-loss rewards on
larger and competition-size maps. The top DRL agent by \citet{Ferdinand2021doublecone} in the Lux AI Season 2 competition used a "DoubleCone" neural
network backbone with critic and actor heads. DoubleCone is similar to ResNet's backbone but the 
middle residual blocks are downscaled 4x to reduce inference time.
\agentName\ switches from shaped to sparse rewards during training and uses the DoubleCone architecture.


\section{Methods}
\label{sec:methods}
\subsection{Neural network architecture}
We reimplement much of MicroRTS-Py, including the PPO implementation, action
composition, shaped reward function, invalid action masking, GridNet, self-play, and
scripted bot training. \agentName's codebase\footnote{\rlAlgoImplsGitHubUrl} supports 
multiple environments and reimplementing allowed the environment to fit into the 
existing codebase. We extended the observation representation in two ways:
\begin{inparaenum}[(1)]
    \item walls and
    \item unit destinations as invalid move targets in the invalid action mask.
\end{inparaenum}
Only the unit's current location was considered invalid in MicroRTS-Py; however, 
microRTS doesn't allow units to move into where another unit is moving into.
Padded positions were represented as walls, which are impassable and noninteractive. 
We also fixed a bug where resources (which are unowned) were being counted as
owned by the opponent if the agent was the second
player\footnote{\unownedFixGitHubCommit}.

The submission version of the Python process loads 7 different policy networks, only one network is used
for a given map (Table~\ref{tab:policy-networks}). Networks are chosen by prioritizing
the compatible networks and then picking the highest priority network that is capable of
running within the allotted time on the current hardware. Map-specific networks are
prioritized higher than size-specific networks. Multiple networks can be prioritized for
the same map or map size. While microRTS supports any map size (even non-square),
observations are padded to fit the policy network. Policy actions are clipped to fit the
map size.

\agentName\ uses two different neural network backbones:
\citet{Ferdinand2021doublecone}'s DoubleCone(4, 6, 4) and a custom network
squnet. The actor head is a convolutional layer
that outputs logits for unit actions at every position. A unit action is composed of
independent discrete subactions: $D = \{a_{action\ type},$ $a_{move\ direction},$ $a_{harvest\
direction},$ $a_{return\ direction},$ $a_{produce\ direction},$ $a_{produce\ type},$ $a_{relative\
attack\ position}\}$. This amounts to 78 logits per position. Invalid action masking sets
logits to a very large negative number (thus zeroing probabilities and gradients) for actions that are
illegal or would accomplish nothing (e.g., moving a unit to an occupied or reserved
position) \citep{DBLP:journals/corr/abs-2006-14171}. This masking significantly reduces
the action space per turn and makes training more efficient.

While DoubleCone could support any map size, inference would likely exceed 100
milliseconds for larger maps. Therefore, for larger maps, we used a different
architecture, squnet, which nests 3 downscaling blocks (Figure~\ref{fig:squnet}). This 
creates a network shaped like U-Net, but functionally similar to DoubleCone.
This aggressive downscaling reduces the number of operations necessary during inference,
especially for larger maps (Table~\ref{table:architectureBreakdown}).

Instead of 1 value head, \agentName\ uses 3 values heads for 3 different value
functions:
\begin{inparaenum}[(1)]
    \item shaped reward similar to MicroRTS-Py except each combat unit type is
    scaled by build-time (rewarding expensive units more),
    \item win-loss sparse reward at game end (Tanh activation), and
    \item in-game difference in units based on cost (similar to the reward function used
    by \citet{Winter2021}).
\end{inparaenum}
These 3 value heads are used to mix-and-match rewards over the course of training,
generally starting with dense rewards using heads (1) and (3) and finishing with only
win-loss sparse rewards by the end.

\subsection{Base model training}
We train using the PPO loss function from \citet{DBLP:journals/corr/SchulmanWDRK17}:
\begin{align}
    L^{CLIP+VF+S}(\theta) &= \hat{\mathbb{E}}_t \left[ L^{CLIP}(\theta) - c_1 L^{VF}(\theta) + c_2 S\left[\pi_\theta\right](s_t) \right], \\
    L^{CLIP}(\theta) &= \min \left( \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)} \hat{A}_t, \text{clip}\left( \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}, 1-\epsilon, 1+\epsilon \right) \hat{A}_t \right), \\
    L^{VF}(\theta) &= \frac{1}{2} \left( V_\theta(s_t) - \hat{V}_t \right)^2,
\end{align}
where $c_1$ is the value loss coefficient, $c_2$ is the entropy coefficient, $S$ is an
entropy bonus function, $\pi_\theta$ is the stochastic policy, $\pi_{\theta_{old}}$ is the rollout policy, $a_t$
is the action taken at time $t$, $s_t$ is the observation at time $t$, $\hat{A}_t$
is the advantage estimate,  $\epsilon$ is the clipping coefficient, $V_\theta$ is the
value function, $\hat{V}_t$ is the return estimate. We estimate the advantage
using Generalized Advantage Estimation (GAE) \citep{DBLP:journals/corr/SchulmanMLJA15}.
We compute a separate advantage for each of the 3 rewards with independent $\gamma$
(discount factor) and $\lambda$ (exponential weight discount) for each reward. The 3 
advantages are weighted summed together to get the final advantage estimate, which is
used in computing the policy loss ($L^{CLIP}$).

The reward weights, value loss coefficients (each value
head has its own loss coefficient), entropy coefficient, and learning rate are varied on
a schedule. At the start of training, the policy loss heavily weighs the shaped 
reward advantage, and the value loss similarly weighs the shaped value head. By the end
of training, both losses are weighted more towards the win-loss sparse reward. The entropy coefficient is also
lowered at the end of training to discourage the agent from making random moves as the
learning rate is lowered. The schedule specifies phases where these values are set and
transitions between phases where values are changed linearly based on timesteps.

We first trained \agentName\ with self-play both against itself and against prior
versions of itself (Table~\ref{tab:training-parameters},
Table~\ref{tab:initial-training-schedule}). We reached a 91\% win rate against the same bots MicroRTS-Py was
benchmarked against. However, it only beat CoacAI (the 2020 competition winner) in 20\%
of games. The best performing agent of MicroRTS-Py nearly always beat CoacAI; however,
the best versions using GridNet also usually lost against CoacAI. We fine-tuned
the model through 3 iterations:
\begin{inparaenum}[(1)]
    \item one-half of environments trained against CoacAI (Table~\ref{tab:shaped-finetuning-schedule});
    \item one-half of environments trained against CoacAI or Mayari (2021 competition
    winner) split evenly and primarily trained on win-loss rewards (Table~\ref{tab:sparse-finetuning-schedule}); and
    \item same as before with action mask improvements and a GELU activation
    after the stride-4 convolution to match \citet{Ferdinand2021doublecone}'s DoubleCone.
\end{inparaenum}
By the end of fine-tuning, the model was winning 98\% of games, including about 90\%
against each of CoacAI and Mayari.

\subsection{Transfer learning}
The model so far had been trained on the 5 smaller Open maps. Using the
fine-tuned model as a starting point, we trained additional models each trained exclusively
on 1 of 3 Open maps. \texttt{NoWhereToRun9x8} is very different from the other maps with a wall of
resources separating opponents. \texttt{DoubleGame24x24} and
\texttt{BWDistantResources32x32} are larger than the maps the base model trained on.
All 3 transfer learning runs used the same schedule:
\begin{inparaenum}[(1)]
    \item warmup of sparse, win-loss reward weights linearly transitioned to a mixture
    of both shaped and sparse rewards,
    \item middle phase of mixed rewards,
    \item end phase of sparse rewards again at a lower learning rate (Table~\ref{tab:transfer-learning-schedule}).
\end{inparaenum}
\texttt{NoWhereToRun9x8} had an additional fine-tuning using only the win-loss reward.
These transfer learned agents exceeded 90\% win-rate on their respective maps,
significant improvements over the base model (especially on
\texttt{BWDistantResources32x32} which started below 10\%)

\subsection{squnet training}
We trained the squnet models with fewer steps because of time constraints and skipped the
predominantly cost-based reward phase because it didn't help train the
base model (Table~\ref{tab:squnet-training-parameters}). The 2 models trained from randomly 
initialized weights for maps of up to 32 (map32) and 64 (map64) managed only a 40\% 
win-rate, never beating CoacAI or Mayari. These models were policies of last resort.

We also fine-tuned the squnet-map32 model on \texttt{BWDistantResources32x32}
exclusively using the transfer learning sparse fine-tuning schedule. This fine-tuned 
model achieved 85\% win-rate, beating Mayari half the time, but never beating CoacAI.

\subsection{Behavior cloning bootstrapped training}
In follow-up work after the \agentName\ submission, we wanted to train a model that
\begin{inparaenum}[(1)]
    \item didn't require the shaped rewards and reward scheduling,
    \item could be trained in fewer steps and less time, and
    \item could defeat prior competition winners on the largest maps.
\end{inparaenum}
We opted for a neural architecture between DoubleCone and squnet: a nested downscaling residual
block each of stride 4, so the bottom block scales the input down to 1/16th the original
size. At each downscaling level, there were multiple residual blocks (6 at full
resolution, split evenly by the downscaling block; 4 at 1/4 resolution, split evenly by
the 1/16 downscaling block; and 4 at 1/16 resolution). This architecture theoretically
has a 128x128 receptive field while using 25\% fewer operations than DoubleCone at inference time. On
the largest Open map (\texttt{(4)BloodBath.scmB}), this is 6 times more
computations than squnet-64. Therefore, this neural architecture won't be usable in a
competition given the same hardware constraints as the 2023 competition.

Initially, we tried a similar training strategy to \agentName\ where the model is
trained on 16x16 maps and reused as a starting point to transfer to
larger maps. However, we only managed a 60\% win-rate on
\texttt{BWDistantResources32x32} and less than a 20\% win-rate on
\texttt{(4)BloodBath.scmB} after over 100 million
steps before terminating training.

We then used imitation learning to bootstrap the model, similar to
\cite{Vinyals2019GrandmasterLI}. We got rid of the three rewards, opting for only the
win-loss reward. microRTS doesn't have human replays, so we used playthroughs of the
2021 competition winner Mayari playing against itself, 2020 competition winner CoacAI, 
and POLightRush (baseline scripted bot and 2017 competition winner). Instead of
generating an offline replay dataset, we set the microRTS environment to play bots
against each other and these observations and actions were fed into rollouts used for
behavior cloning the policy and fitting the value heads:
\begin{align}
    L^{BC+VF}(\theta) &= \hat{\mathbb{E}}_t \left[ L^{BC}(\theta) - c_1 L^{VF} \right], \\
    L^{BC}(\theta) &= -\frac{1}{\|a_t\|} \log \pi_{\theta}(a_t|s_t),
\end{align}
where $c_1$ and $L^{VF}$ are the same as in the PPO loss function. The policy behavior
cloning loss is the cross-entropy loss between the policy logits and the actions taken
by the Mayari bot. We found scaling the loss by the number of units that could take an
action critical for behavior cloning to train.

We trained 3 behavior cloned models (16x16, 32x32, and 64x64) on the same maps for each
map size as \agentName\ training. The 64x64 model used the weights of the 32x32 model 
as a starting point, while the other two models were randomly initialized. We then used 
PPO to fine-tune the behavior cloned models on the same maps.

\section{Results}
\subsection{Single player round-robin benchmark}
\label{sec:single-player-benchmark}
\begin{table}[ht]
    \centering
    \begin{threeparttable}
    \caption{Single player round-robin benchmark win rates. Win rates above 50\% are bolded. (D)
    DoubleCone model used. (S) squnet model used.}
    \label{tab:single-player-winrate}
    \begin{tabular}{lcccc|c}
     & \textbf{WorkerRush} & \textbf{LightRush} & \textbf{CoacAI} & \textbf{Mayari} & \textbf{Overall} \\
    \midrule
    \texttt{basesWorkers8x8A} & \textbf{95} & \textbf{100} & \textbf{99} & \textbf{100} & \textbf{99} \\
    \texttt{FourBasesWorkers8x8} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{98} & \textbf{100} \\
    \texttt{NoWhereToRun9x8} & \textbf{100} & \textbf{100} & \textbf{93} & \textbf{99} & \textbf{98} \\
    \texttt{basesWorkers16x16A} & \textbf{100} & \textbf{100} & \textbf{90} & \textbf{98} & \textbf{97} \\
    \texttt{TwoBasesBarracks16x16} & \textbf{100} & \textbf{89} & \textbf{99} & \textbf{100} & \textbf{97} \\
    \texttt{DoubleGame24x24} & \textbf{100} & \textbf{98} & \textbf{94} & \textbf{100} & \textbf{98} \\
    \texttt{BWDistantResources32x32} & \begin{tabular}[c]{@{}c@{}}\textbf{99} (D) \\ \textbf{93}
    (S)\end{tabular}  & \begin{tabular}[c]{@{}c@{}}\textbf{90} (D) \\ \textbf{73}
    (S)\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{88} (D) \\ 23
    (S)\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}\textbf{99} (D) \\ \textbf{58} (S)\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}\textbf{94}\tnote{\P} (D) \\ \textbf{61} (S)\end{tabular} \\
    \texttt{(4)BloodBath.scmB} & \textbf{98} & 0 & 0 & 0 & 25\tnote{\dag} \\
    \hline
    \textbf{AI Average}\tnote{*} & \textbf{99} & \textbf{85} & \textbf{83} & \textbf{87} & \textbf{88} \\
    \end{tabular}
    \begin{tablenotes}
    \item[*] AI Average uses the DoubleCone (D) results from \texttt{BWDistantResources32x32}.
    \item[\P] \agentName\ lost 0.25\% of matches (1 match) by timeout.
    \item[\dag] \agentName\ lost  1\% of matches (4 matches) by timeout.
    \end{tablenotes}
    \end{threeparttable}
\end{table}

In a single player round-robin benchmark on the Open maps (Table~\ref{tab:single-player-winrate}), \agentName\
beat the competition winners of 2021 (Mayari), 2020 (CoacAI), and 2017 (POLightRush,
baseline) on 7 of the 8 maps (winning over 96\% of games on these maps). \agentName\
could only beat the POWorkerRush baseline bot on the largest map,
\texttt{(4)BloodBath.scmB}. The DistantResources fine-tuned squnet model performed worse than the DoubleCone model
across all opponents, but maintained an over 50\% win rate against all but CoacAI. Timeouts didn't affect results significantly.

\subsection{IEEE-CoG2023 microRTS competition results}
The IEEE-CoG2023 microRTS competition is a round-robin tournament on 12 maps of
different sizes and distributions of terrain, resources, and starting units and
buildings. 8 Open maps are known beforehand, 4 Hidden maps are only revealed after the
competition. The winner is the agent with the highest win rate on the 8 Open maps.
Hidden map results were released for the competition, but this paper will only discuss
the Open maps. For
this competition, a total of 11 agents were submitted: 9 programmatic policies, 1
synthesized programmatic policy, and \agentName. The competition also had 6
baselines:
\begin{inparaenum}[(1)]
    \item RandomBiasedAI (performs actions randomly, biased towards attacking if able),
    \item NaiveMCTS (a simple Monte Carlo tree search agent that searches until reaching
    the time limit)
    \item POWorkerRush,
    \item POLightRush,
    \item 2L (programmatic strategies generated by \citet{Moraes2023ChoosingWY}, the
    competition organizers), and
    \item the prior competition winner Mayari.
\end{inparaenum}
The baselines cannot win the competition.

\agentName\ was declared the winner with the highest win rate (72\%) across all
submissions (Table~\ref{tab:competition-winrate}). \agentName\ had a higher win rate 
than all but two baselines: \texttt{2L} (76\%) and Mayari (82\%). Despite placing below 
two baselines, \agentName\ had an over 50\% win rate versus every opponent including 
\texttt{2L} (60\%) and Mayari (65\%).

\begin{table}[ht]
    \caption{Win rates of selection of agents in the IEEE-CoG2023 microRTS competition.
    Player 1 is the row agent and player 2 is the column agent. Each win rate value is
    the percentage of games won by player 1. Cells are bolded if the win
    rate is higher than the opponent's row win rate. For example, \agentName vs
    ObiBotKenobi is bolded because 49\% is higher than 47\%, thus meaning the combined player 1
    and 2 win rate is 51\% for \agentName vs ObiBotKenobi. Overall includes all
    agents, including those not shown. Win rates for all agents are shown in
    Table~\ref{tab:all-competition-winrate}.}
    \label{tab:competition-winrate}
    \begin{center}
    \begin{tabular}{lcccccc|c}
    & \begin{sideways} Mayari \end{sideways} 
    & \begin{sideways} 2L \end{sideways} 
    & \begin{sideways} \textbf{\agentName} \end{sideways} 
    & \begin{sideways} ObiBotKenobi \end{sideways} 
    & \begin{sideways} POLightRush \end{sideways} 
    & \begin{sideways} POWorkerRush \end{sideways} 
    & \begin{sideways} Overall \end{sideways} \\
    \midrule
    Mayari (2021 winner) & - & \textbf{53} & 32 & \textbf{73} & \textbf{88} & \textbf{75} & 82 \\
    2L (baseline) & 51 & - & 39 & \textbf{50} & \textbf{75} & \textbf{88} & 76 \\
    \textbf{\agentName} (2023 winner) & \textbf{62} & \textbf{59} & - & \textbf{49} & \textbf{64} & \textbf{78} & 72 \\
    ObiBotKenobi (2023 2nd place) & 39 & 29 & 47 & - & \textbf{58} & \textbf{65} & 66 \\
    POLightRush (baseline) & 0 & 25 & 29 & 38 & - & \textbf{69} & 55 \\
    POWorkerRush (baseline) & 13 & 13 & 21 & 29 & 38 & - & 53 \\
    \end{tabular}
\end{center}
\end{table}

\begin{table}[ht]
    \caption{\agentName\ win rates in 2023 competition by opponent and map. Bolded cells are win rates over 50\%. Overall includes all agents, including those not shown.}
    \label{tab:competition-winrate-by-map}
    \begin{center}
    \begin{tabular}{lccccc|c}
    & \begin{sideways}POWorkerRush\end{sideways} & \begin{sideways}POLightRush\end{sideways} & \begin{sideways}ObiBotKenobi\end{sideways} & \begin{sideways}2L\end{sideways} & \begin{sideways}Mayari\end{sideways} & \begin{sideways}Overall\end{sideways}\\ 
    \midrule
    \texttt{basesWorkers8x8A} & \textbf{60} & \textbf{70} & \textbf{60} & \textbf{60} & \textbf{60} & \textbf{66}\\ 
    \texttt{FourBasesWorkers8x8} & \textbf{100} & \textbf{100} & 21 & \textbf{97} & \textbf{100} & \textbf{95}\\ 
    \texttt{NoWhereToRun9x8} & \textbf{90} & \textbf{85} & \textbf{83} & \textbf{70} & \textbf{70} & \textbf{84}\\
    \texttt{basesWorkers16x16A} & \textbf{100} & \textbf{100} & \textbf{95} & \textbf{100} & \textbf{100} & \textbf{100}\\
    \texttt{TwoBasesBarracks16x16} & \textbf{80} & \textbf{80} & 10 & \textbf{70} & \textbf{80} & \textbf{75}\\
    \texttt{DoubleGame24x24} & \textbf{80} & \textbf{75} & \textbf{78} & \textbf{80} & \textbf{75} & \textbf{80}\\ 
    \texttt{BWDistantResources32x32} & 50 & 30 & 35 & 3 & 35 & \textbf{54}\\
    \texttt{(4)BloodBath.scmB} & \textbf{70} & 0 & 28 & 0 & 0 & 34\\ 
    \hline
    AI Average & \textbf{79} & \textbf{68} & \textbf{51} & \textbf{60} & \textbf{65} & \textbf{74}\\ 
    \end{tabular}
    \end{center}
\end{table}

As expected from the single player round-robin benchmark, \agentName\ does better on
smaller maps and dismally on the largest maps
(Table~\ref{tab:competition-winrate-by-map}). However, in the competition, \agentName\
underperformed against agents already benchmarked in the single player round-robin
(14-19\% lower win rate against each agent), even accounting for the likely use of
the weaker squnet model on \texttt{BWDistantResources32x32}.  Breaking down by map, 
\agentName\ underperformed against benchmarked agents by 20-40\% on 5 maps.

The competition ran the tournament in jobs splitting each map into 5 or 10 jobs where
each job would run a complete round-robin with all agents on that map playing 2 or 1
games as player 1 and 2 each. For \texttt{basesWorkers8x8A}, on which \agentName\
underperformed by almost 40\%, the competition had 5 jobs. On the first 3 jobs,
\agentName\ won nearly every game. On the last 2 jobs, \agentName\ lost nearly every
game. 1 or 2 jobs per underperforming map appear to have outlier low win rates
for \agentName\ (Table~\ref{tab:outlier-winrate}).

\subsection{Behavior cloning results}
\label{sec:behavior-cloning-results}
We created 2 additional agents from the behavior cloning (\bcAgent) and the following
PPO fine-tuning (\bcPPOAgent). Each agent consisted of the models trained on their
respective map sizes (16x16, 32x32, and 64x64). These agents do not have any
map-specific models.

\begin{table}[ht]
    \centering
    \caption{\bcPPOAgent\ win rate in single player round-robin benchmark. Win rates above 50 are bolded.}
    \label{tab:bcppo-winrate}
    \begin{center}
    \begin{tabular}{lcccc|c}
    & POWorkerRush & POLightRush & CoacAI & Mayari & Overall \\
    \midrule
    \texttt{basesWorkers8x8A} & \textbf{92} & \textbf{100} & \textbf{85} & \textbf{100} & \textbf{94} \\
    \texttt{FourBasesWorkers8x8} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} \\
    \texttt{NoWhereToRun9x8} & \textbf{100} & \textbf{100} & \textbf{90} & \textbf{80} & \textbf{92} \\
    \texttt{basesWorkers16x16A} & \textbf{100} & \textbf{100} & \textbf{95} & \textbf{95} & \textbf{98} \\
    \texttt{TwoBasesBarracks16x16} & \textbf{100} & 0 & \textbf{100} & \textbf{95} & \textbf{74} \\
    \texttt{DoubleGame24x24} & \textbf{98} & \textbf{85} & \textbf{100} & \textbf{100} & \textbf{96} \\
    \texttt{BWDistantResources32x32} & \textbf{100} & \textbf{100} & \textbf{95} & \textbf{100} & \textbf{99} \\
    \texttt{(4)BloodBath.scmB} & \textbf{100} & \textbf{88} & 0 & 5 & 48 \\
    \hline
    AI Average & \textbf{99} & \textbf{84} & \textbf{83} & \textbf{84} & \textbf{88} \\
    \end{tabular}
    \end{center}
\end{table}

\bcAgent\ had a 71\% win rate, doing well against the POLightRush baseline (96\%, better than \agentName) and respectably against
Mayari (44\%) (Table~\ref{tab:bc-winrate}). On the largest map, \bcAgent\ manages to occasionally beat POLightRush (63\%), CoacAI
(20\%), and Mayari (40\%) compared to \agentName's 0\% across all 3 opponents.

Once fine-tuned with PPO, \bcPPOAgent\ obtains an \agentName-comparable 88\% win rate
(Table~\ref{tab:bcppo-winrate}). \bcPPOAgent\ generally improves upon \bcAgent's win
rates on each map and against each opponent. However, the biggest exceptions are
POLightRush on \texttt{TwoBasesBarracks16x16} (from 100\% to 0\%) and the largest map
where the fine-tuned model can no longer beat CoacAI and Mayari.

\section{Discussion}
\subsection{Improving inference time in microRTS competitions}
\agentName's underperformance in the 2023 competition on 6 maps suggests that the job
environment could sometimes run slower. We worked with the competition
organizers to reduce the chance of timeouts. However, it was difficult to reproduce the same results as
the competition servers in our development environments.

Improving inference time is critical to reproducing benchmark results in a competition.
We suggest 3 improvements (2 for agents, and 1 for the competition organizers):
\begin{inparaenum}[(1)]
    \item use fast inference runtime providers like OpenVINO for ONNX Runtime,
    \item continue to train agents using the smaller squnet models (possibly with
    behavior cloning to bootstrap training), and
    \item replace the fixed per turn timeout tolerance in the competition with an overtime
    budget.
\end{inparaenum}
For DoubleCone, \citet{Ferdinand2021doublecone} found using OpenVINO could have made
inference 2-3 times faster in the LUX competition. This  would likely
make running DoubleCone or the larger squnet models on all maps except the largest
feasible for the competition. An overtime budget for an entire match instead of the 20 ms
per turn tolerance will help agents deal with environment instabilities. For example,
\bcPPOAgent\ timed out in 11\% of games on \texttt{(4)BloodBath.scmB}, despite averaging
55 ms/turn and going over 100 ms in only 0.016\% of turns (averaging less than 1 over
100 ms turn per game). An overage budget of even 1 second per game would likely prevent
most timeouts.

\subsection{Training on larger maps}
None of our agents managed to reliably defeat the prior two competition winners on 
the largest map. \texttt{(4)BloodBath.scmB} is a challenging map for DRL because game 
lengths are significantly longer than on smaller
maps. \bcPPOAgent\ averaged 3,500 steps per game on
\texttt{(4)BloodBath.scmB} compared to around 925 steps on the next largest map,
\texttt{BWDistantResources32x32}. DRL must learn to propagate rewards over longer time
periods, and the observation-action space is so large that DRL can only hope to explore a
fraction of it. A rushing strategy of sending attack units and surplus workers towards
the enemy as soon as possible is a strong strategy on all but the largest map.
\texttt{(4)BloodBath.scmB} seems to reward a build up of forces before attacking, which
DRL struggled to learn.

We hoped imitation learning would mitigate these issues by providing a model that
generates non-zero win-loss rewards and reasonable observation-action pairs.
However, during PPO fine-tuning, a training policy that initially won 40-50\% of
training games, dropped to 20\% midway through training. It eventually recovers to
winning 40\% of training games; however, the fine-tuned policy had a worse evaluation
win rate than the initial supervised policy. This training curve differs from the
smaller map fine-tuning where the training policy quickly won 60\% of
training games and improved upon the evaluation win rate.

PPO (and DRL algorithms in general) have many hyperparameters that need to be tuned
before a model is able to learn, and our hyperparameter search is ongoing. Other 
algorithms might perform better from a behavior cloned start. For example,
Advantage Actor Critic (A2C) \citep{DBLP:journals/corr/MnihBMGLHSK16} uses a loss
function similar to behavior cloning, and thus policy training might more smoothly
transition from behavior cloning to DRL.

\subsection{Behavior cloning, transfer learning, and academic competitions}
Training multiple models for \agentName\ took 70 GPU-days. Imitation learning \bcAgent\
trained for 23 GPU-days, and PPO fine-tuning \bcPPOAgent\ took another 49 GPU-days. 
These are significant amounts of compute for a mostly academic competition.

There are several ways to make DRL more feasible in a competition and educational setting:
\begin{inparaenum}[(1)]
    \item focus on smaller maps,
    \item fine-tue pre-trained models (possibly through BC, DRL, or some other method),
    \item transfer an existing model to new maps, or
    \item use a significantly smaller neural architecture.
\end{inparaenum}
The largest map took 19 GPU-days to train for \agentName, 15 GPU-days for \bcAgent, and
34 GPU-days for \bcPPOAgent. Over two-thirds of training time for \bcPPOAgent\ was spent
training on the largest map to little benefit. \citet{DBLP:journals/corr/abs-2105-13807}
trained an agent for player 1 on a single 16x16 map in 60 hours. \bcPPOAgent\ trained
on the 5 Open maps up to size 16x16 in about 7.5 days.

Fine-tuning and transfer learning was critical to making \agentName\ competitive. Both
took significantly less time than training from randomly initialized weights because the
policy already makes reasonable tactical actions and the critic already makes
reasonable value estimates. If future competitions change the Open maps, fine-tuning and transfer learning
will significantly help \agentName\ and \bcPPOAgent\. We didn't train \bcPPOAgent\ on
specific maps, so fine-tuning there could improve win rates. Novel behaviors can be
trained through behavior cloning other agents, such as the new baseline \texttt{2L}.

\agentName's DoubleCone and \bcPPOAgent's deep squnet are relatively large neural
networks, each at around 5 million parameters.
\citet{DBLP:journals/corr/abs-2105-13807}'s best performing policies each used fewer
than 1 million parameters. These smaller networks are quicker to train and have faster
inference time, and there's no definitive evidence so far that they are worse than
larger networks.

\section{Conclusion}
\agentName\ is the first DRL agent to win a microRTS competition. It demonstrates that an
iterative training process of fine-tuning and transfer learning is effective for
creating competitive DRL agents. Such a training process can be used by 
resource-constrained researchers and students to create novel DRL agents for future 
competitions and experiments. Fine-tuning behavior cloning with PPO is a promising way
to create competitive DRL agents with desired behaviors without needing to handcraft
shaped reward functions.

\section{Reproducibility Statement}
Our Methods section describes the architecture and training process for \agentName,
\bcAgent, and \bcPPOAgent. We give additional details in Appendices:
\begin{inparaenum}[]
    \item Appendix~\ref{appendix:architecture} neural network architectures,
    \item Appendix~\ref{appendix:initial-training-details} initial training details,
    \item Appendix~\ref{appendix:transfer-learning-details} transfer learning details,
    \item Appendix~\ref{appendix:squnet-learning-details} squnet learning details, and
    \item Appendix~\ref{appendix:behavior-cloning-details} behavior cloning details.
\end{inparaenum}
Appendix~\ref{appendix:single-player-benchmark-setup} describes the setup of the
single player round-robin tournaments for \agentName\
(Section~\ref{sec:single-player-benchmark}), \bcAgent, and \bcPPOAgent\ (Section~\ref{sec:behavior-cloning-results}).
The microRTS competition requires agents to be open sourced. Our open-sourced code
repository includes a link to the archive used for the competition submission and
instructions on how to run the submission in the competition
environment\footnote{\raiMicroRTSGitHubUrl}.

\bibliography{iclr2024}
\bibliographystyle{iclr2024_conference}

\clearpage

\appendix
\section{Competition details}
\begin{table}[H]
    \centering
    \begin{threeparttable}
        \caption{Open competition maps. Representation column is the size of the vectorized observation in \agentName.}
        \label{tab:open-maps}
            \begin{tabular}{>{\centering\arraybackslash}m{4cm}>{\centering\arraybackslash}m{2cm}>{\centering\arraybackslash}m{2cm}>{\centering\arraybackslash}m{2.25cm}}
                \multicolumn{1}{c}{\bf Name}  &\multicolumn{1}{c}{\bf Size}
                &\multicolumn{1}{c}{\bf Representation}
                &\multicolumn{1}{c}{\bf Start} \\ \hline
                basesWorkers8x8A & 8x8 & 16x16 &
                \includegraphics[width=2.25cm]{supplemental/vsMayari/basesWorkers8x8A.png} \\
                FourBasesWorkers8x8 & 8x8 & 16x16 &
                \includegraphics[width=2.25cm]{supplemental/vsMayari/FourBasesWorkers8x8.png} \\
                NoWhereToRun9x8 & 9x8 & 12x12\tnote{*} or 16x16\tnote{\P} &
                \includegraphics[width=2.25cm]{supplemental/vsMayari/NoWhereToRun9x8.png} \\
                basesWorkers16x16A & 16x16 & 16x16 &
                \includegraphics[width=2.25cm]{supplemental/vsMayari/basesWorkers16x16A.png} \\
                TwoBasesBarracks16x16 & 16x16 & 16x16 &
                \includegraphics[width=2.25cm]{supplemental/vsMayari/TwoBasesBarracks16x16.png}
                \\
                DoubleGame24x24 & 24x24 & 24x24\tnote{\dag} or 32x32\tnote{\ddag} &
                \includegraphics[width=2.25cm]{supplemental/vsMayari/DoubleGame24x24.png} \\
                BWDistantResources32x32 & 32x32 & 32x32 &
                \includegraphics[width=2.25cm]{supplemental/vsMayari/BWDistantResources32x32.png}
                \\
                (4)BloodBath.scmB & 64x64 & 64x64 &
                \includegraphics[width=2.25cm]{supplemental/vsMayari/(4)BloodBath.png} \\
            \end{tabular}
            \begin{tablenotes}
                \item[*] ppo-Microrts-finetuned-NoWhereToRun-S1-best uses a 12x12
                representation
                \item[\P] ppo-Microrts-A6000-finetuned-coac-mayari-S1-best
                \item[\dag] ppo-Microrts-finetuned-DoubleGame-shaped-S1-best uses a 24x24
                    representation.
                \item[\ddag] ppo-Microrts-squnet-map32-128ch-selfplay-S1-best pads the
                    observation to 32x32.
            \end{tablenotes}
    \end{threeparttable}
\end{table}

\begin{table}[h]
    \caption{Policy networks used by \agentName}
    \label{tab:policy-networks}
    \begin{center}
        \begin{tabular}{|p{0.5\linewidth}|p{0.5\linewidth}|}
            \multicolumn{1}{c}{\bf Network}  &\multicolumn{1}{c}{\bf Usage} \\
            \hline
            ppo-Microrts-finetuned-NoWhereToRun-S1-best & NoWhereToRun9x8 \\ \hline
            ppo-Microrts-A6000-finetuned-coac-mayari-S1-best & All other maps of size 16x16 and smaller \\ \hline
            ppo-Microrts-finetuned-DoubleGame-shaped-S1-best & DoubleGame24x24 \\ \hline
            ppo-Microrts-finetuned-DistantResources-shaped-S1-best & BWDistantResources32x32 if completion time under 75 ms \\ \hline
            ppo-Microrts-squnet-DistantResources-128ch-finetuned-S1-best & BWDistantResources32x32 if completion time above 75 ms \\ \hline
            ppo-Microrts-squnet-map32-128ch-selfplay-S1-best & All other maps where longest dimension is between 17-32 \\ \hline
            ppo-Microrts-squnet-map64-64ch-selfplay-S1-best & Maps where the longest
            dimension is over 32 \\
            \hline
        \end{tabular}
    \end{center}
\end{table}

To participate in the competition, \agentName\ has a Java class that handles turn
handling and resetting commands from the Java game engine. While earlier Python
solutions  passed JSON or XML data over a
socket\footnote{\url{https://github.com/douglasrizzo/python-microRTS}}, \agentName\
passes binary data over a pipe to the Python process as a performance optimization for
the larger maps.

Each agent played every other agent on each map 20 times (10 each as player 1 and 2).
Timeouts were disabled for the competition, but the Java-side of \agentName\ would skip
its turn (submitting no actions) if 100 ms had elapsed. On
\texttt{BWDistantResources32x32}, \agentName\ chose between the DoubleCone and squnet
fine-tuned models by running both models on the first observation 100 times each and
choosing DoubleCone if it computed actions within 75 ms on average.


\section{Neural network architecture}
\label{appendix:architecture}
DoubleCone(4, 6, 4) (\citep{Ferdinand2021doublecone}) consists of
\begin{inparaenum}[(1)]
    \item 4 residual blocks;
    \item a downscaled residual block consisting of a stride-4 convolution, 6 residual blocks, and
        2 stride-2 transpose convolutions;
    \item 4 residual blocks; and
    \item actor and value heads (Figure~\ref{fig:doublecone}).
\end{inparaenum}
Each residual block includes a squeeze-excitation layer after the second convolutional
layer (Figure~\ref{fig:squeezeexcitation}).  The values heads are each 
\begin{inparaenum}[(1)]
    \item 2 stride-2 convolutions,
    \item an adaptive average pooling layer,
    \item flattened,
    \item 2 densely connected layers, and
    \item an activation function (Identity [no activation] or Tanh) to a single, scalar value (Figure~\ref{fig:valueheads}).
\end{inparaenum}
The adaptive average pooling layer allows the network to be used on different map sizes.

\begin{figure}[H]
    \begin{center}
        \includegraphics[width=0.59\linewidth]{figures/DoubleCone.png}
    \end{center}
    \caption{DoubleCone(4, 6, 4) neural network architecture.}
    \label{fig:doublecone}
\end{figure}

\begin{figure}[H]
    \begin{center}
        \includegraphics[width=0.3\linewidth]{figures/SqueezeExcitation.png}
    \end{center}
    \caption{ResBlock used in DoubleCone, squnet32, and squnet64. The residual block is similar to a standard residual block but inserts a Squeeze-Excitation block after the convolutional layers and before the residual connection.}
    \label{fig:squeezeexcitation}
\end{figure}

\begin{figure}[H]
    \begin{center}
        \includegraphics[width=0.7\linewidth]{figures/ValueHeads.png}
    \end{center}
    \caption{Value heads used in (from left to right) DoubleCone, squnet32, and
    squnet64. The AdaptiveAvgPool2d layer allows the network to be used on other map
    sizes.}
    \label{fig:valueheads}
\end{figure}

\begin{figure}[H]
    \begin{center}
        \includegraphics[width=0.55\linewidth]{figures/squnet64.png}
    \end{center}
    \caption{squnet64 neural network architecture. Instead of one downscaling block as in DoubleCone, this network downscales 3 times. This aggressive downscaling reduces the number of computations for larger maps, while theoretically supporting a large receptive field.}
    \label{fig:squnet}
\end{figure}
        
\begin{table}[h]
    \centering
    \begin{threeparttable}
        \caption{Comparison of different architectures}
        \label{table:architectureBreakdown}
        \begin{tabular}{cccc}
            & \textbf{DoubleCone} & \textbf{squnet-map32\tnote{\P}} & \textbf{squnet-map64} \\ [0.5ex] 
            \hline
            Levels & 2 & 4 & 4 \\
            \hline
            Encoder residual blocks/level & [4, 6] & [1, 1, 1, 1] & [1, 1, 1, 1] \\
            \hline
            Decoder residual blocks/level & [4] & [1, 1, 1] & [1, 1, 1] \\
            \hline
            Stride/level & [4] & [2, 2, 4] & [2, 4, 4] \\
            \hline
            Deconvolution strides/level & [[2, 2]\tnote{*}] & [2, 2, 4] & [2, 4, 4] \\
            \hline
            Channels/level & [128, 128] & [128, 128, 128, 128] & [64, 64, 64, 64] \\
            \hline
            Trainable parameters & 5,014,865 & 3,584,657 & 1,420,625 \\
            \hline
            MACs\tnote{\dag} & \begin{tabular}[c]{@{}c@{}}0.70B (16x16)\tnote{\ddag} \\ 0.40B (12x12)\tnote{\S} \\ 1.58B (24x24) \\ 2.81B (32x32)\end{tabular} & 1.16B (32x32) & 1.41B (64x64) \\ 
            \hline
        \end{tabular}
        \begin{tablenotes}
            \item[\P] Used by ppo-Microrts-squnet-DistantResources-128ch-finetuned-S1-best and ppo-Microrts-squnet-map32-128ch-selfplay-S1-best. 
            \item[*] 2 stride-2 transpose convolutions to match the 1 stride-4 convolution.
            \item[\dag] Multiply-Accumulates for computing actions for a single observation.
            \item[\ddag] All maps smaller than 16x16 (except NoWhereToRun9x8) are padded with walls up to 16x16. 
            \item[\S] NoWhereToRun9x8 is padded with walls up to 12x12.
        \end{tablenotes}
    \end{threeparttable}
\end{table}

\section{Initial training details}
\label{appendix:initial-training-details}
\agentName\ was trained with partial observability and environment non-determinism disabled.

\begin{table}[H]
    \centering
    \begin{threeparttable}
    \caption{Comparison of initial training, shaped fine-tuning, and sparse fine-tuning parameters}
    \label{tab:training-parameters}
    \begin{tabular}{lccc}
    \toprule
    Parameter & Initial Training & Shaped Fine-Tuning & Sparse Fine-Tuning \\
    \midrule
    Steps & 300M &   100M &   100M \\
    n\_envs & 24 &    \textquotedbl &    \textquotedbl \\
    Rollout Steps Per Env & 512 &   \textquotedbl &   \textquotedbl \\
    Minibatch Size & 4096 &   \textquotedbl &   \textquotedbl \\
    Epochs Per Rollout & 2 &   \textquotedbl &   \textquotedbl \\
    Gamma & [0.99, 0.999, 0.999]\tnote{*} &   \textquotedbl &   \textquotedbl \\
    gae\_lambda & [0.95, 0.99, 0.99]\tnote{\P} &   \textquotedbl &   \textquotedbl \\
    Clip Range & 0.1 &   \textquotedbl &   \textquotedbl \\
    Clip Range VF & 0.1 &    \textquotedbl &   \textquotedbl \\
    VF Coef Halving\tnote{‡} & True  &    \textquotedbl &   \textquotedbl \\
    Max Grad Norm &  0.5 &   \textquotedbl &   \textquotedbl \\   
    Latest Self-play Envs   &   12 &                      \textquotedbl &                      \textquotedbl \\
    Old Self-play Envs   &   12 &                      0 &                      0 \\
    Bots   &   none & CoacAI: 12 & \begin{tabular}[c]{@{}c}CoacAI: 6\\ Mayari: 6\end{tabular} \\
    Maps   &   \begin{tabular}[c]{@{}c}basesWorkers16x16A \\ TwoBasesBarracks16x16 \\
    basesWorkers8x8A \\ FourBasesWorkers8x8 \\ NoWhereToRun9x8 \\
    EightBasesWorkers16x16\tnote{\dag} \end{tabular} &  \textquotedbl & \textquotedbl \\
    \bottomrule
    \end{tabular}
    \begin{tablenotes}
    \item[\textquotedbl] Same value as cell to left.
    \item[*] Value per value head (shaped, win-loss, cost-based).
    \item[\P] Multiply v\_{loss} by 0.5, as done in CleanRL.
    \item[\dag] Map not used in competition.
    \end{tablenotes}
    \end{threeparttable}
\end{table}

\begin{table}[H]
    \centering
    \begin{threeparttable}
    \caption{Initial training schedule from a randomly initialized model}
    \label{tab:initial-training-schedule}
    \begin{tabular}{lccccr}
    \toprule
     & Phase 1 & Transition 1→2\tnote{*} & Phase 2 & Transition 2→3\tnote{*} & Phase 3 \\
    \midrule
    steps & 90M & 60M & 30M & 60M & 60M \\
    reward weights\tnote{†} & [0.8, 0.01, 0.19] &  & [0, 0.5, 0.5] &  & [0, 0.99, 0.01] \\
    $c_1$ (value loss coef)\tnote{†} & [0.5, 0.1, 0.2] &  & [0, 0.4, 0.4] &  & [0, 0.5, 0.1]\\
    $c_2$ (entropy coef)& 0.01 & & 0.01 & & 0.001 \\
    learning rate & $10^{-4}$ & & $10^{-4}$ & & $5 \times 10^{-5}$\\
    \bottomrule
    \end{tabular}
    \begin{tablenotes}
       \item[*] Values are linearly interpolated between phases based on step count.
       \item[†] Listed weights are for the shaped, win-loss, cost-based values, respectively.
    \end{tablenotes}
    \end{threeparttable}
\end{table}

\begin{table}[H]
    \centering
    \begin{threeparttable}
    \caption{Shaped fine-tuning schedule}
    \label{tab:shaped-finetuning-schedule}
    \begin{tabular}{lccccc}
    \toprule
     & Start & Transition →1\tnote{*} & Phase 1 & Transition 1→2\tnote{*} & Phase 2 \\
     \midrule
    steps & & 5M & 30M & 20M & 45M \\
    reward weights\tnote{†} & [0, 0.99, 0.01] & & [0, 0.5, 0.5] & & [0, 0.99, 0.01] \\
    $c_1$ (value loss coef)\tnote{†} & [0, 0.4, 0.2] & & [0, 0.4, 0.4] & & [0, 0.5, 0.1]\\
    $c_2$ (entropy coef) & 0.01 & & 0.01 & & 0.001 \\
    learning rate & $10^{-5}$ & & $5 \times 10^{-5}$ & & $5 \times 10^{-5}$\\
    \bottomrule
    \end{tabular}
    \begin{tablenotes}
       \item[*] Values are linearly interpolated between phases based on step count.
       \item[†] Listed weights are for the shaped, win-loss, cost-based values, respectively.
    \end{tablenotes}
    \end{threeparttable}
\end{table}

\begin{table}[H]
    \centering
    \begin{threeparttable}
    \caption{Sparse fine-tuning schedule}
    \label{tab:sparse-finetuning-schedule}
    \begin{tabular}{lcccc}
    \toprule
     & Phase 1 & Transition 1→2\tnote{*} & Phase 2 \\
     \midrule
    steps & 30M & 40M & 30M \\
    reward weights\tnote{†} & [0, 0.99, 0.01] &  & [0, 0.99, 0.01] \\
    $c_1$ (value loss coef)\tnote{†} & [0, 0.5, 0.1] &  & [0, 0.5, 0.1]\\
    $c_2$ (entropy coef)) & 0.001 & & 0.0001 \\
    learning rate & $5 \times 10^{-5}$ & & $10^{-5}$ \\
    \bottomrule
    \end{tabular}
    \begin{tablenotes}
       \item[*] Values are linearly interpolated between phases based on step count.
       \item[†] Listed weights are for the shaped, win-loss, cost-based values, respectively.
    \end{tablenotes}
    \end{threeparttable}
\end{table}

\section{Transfer learning details}
\label{appendix:transfer-learning-details}

\begin{table}[H]
    \centering
    \begin{threeparttable}
    \caption{Transfer learning schedule starting from ppo-Microrts-A6000-finetuned-coac-mayari-S1-best model}
    \label{tab:transfer-learning-schedule}
    \begin{tabular}{lccccc}
    \toprule
     & Start & Transition →1\tnote{*} & Phase 1 & Transition 1→2\tnote{*} & Phase 2 \\
     \midrule
    steps & & 5M & 30M & 20M & 45M \\
    reward weights\tnote{†} & [0, 0.99, 0.01] & & [0.4, 0.5, 0.1] & & [0, 0.99, 0.01] \\
    $c_1$ (value loss coef)\tnote{†} & [0.2, 0.4, 0.2] & & [0.3, 0.4, 0.1] & & [0, 0.5, 0.1]\\
    $c_2$ (entropy coef) & 0.01 & & 0.01 & & 0.0001 \\
    learning rate & $5 \times 10^{-5}$ & & $7 \times 10^{-5}$ & & $10^{-5}$ \\
    \bottomrule
    \end{tabular}
    \begin{tablenotes}
       \item[*] Values are linearly interpolated between phases based on step count.
       \item[†] Listed weights are for the shaped, win-loss, cost-based values, respectively.
    \end{tablenotes}
    \end{threeparttable}
\end{table}

\section{squnet learning details}
\label{appendix:squnet-learning-details}

\begin{table}[H]
    \centering
    \begin{threeparttable}
    \caption{Squnet training parameters}
    \label{tab:squnet-training-parameters}
    \begin{tabular}{lccc}
    \toprule
    Parameter & map32 & map32-DistantResources & map64 \\
    \midrule
    Steps & 200M & 100M & 200M \\
    n\_envs & 24 & \textquotedbl & \textquotedbl \\
    Rollout Steps Per Env & 512 & 512 & 256 \\
    Minibatch Size & 2048 & 2048 & 258 \\
    Clip Range & 0.1 & \textquotedbl & \textquotedbl \\
    Clip Range VF & none & \textquotedbl & \textquotedbl \\
    Latest Self-play Envs & 12 & \textquotedbl & \textquotedbl \\
    Old Self-play Envs & 6 & 6 & 4 \\
    Bots & \begin{tabular}[c]{@{}c}CoacAI: 3 \\ Mayari: 3 \end{tabular} & \begin{tabular}[c]{@{}c}CoacAI: 3 \\ Mayari: 3 \end{tabular} & \begin{tabular}[c]{@{}c}CoacAI: 4 \\ Mayari: 4 \end{tabular} \\
    Maps & \begin{tabular}[c]{@{}c}DoubleGame24x24 \\ BWDistantResources32x32 \\ chambers32x32\tnote{*} \end{tabular} & BWDistantResources32x32 & \begin{tabular}[c]{@{}c}BloodBath.scmB \\ BloodBath.scmE\tnote{*}\end{tabular} \\   
    \bottomrule
    \end{tabular}
    \begin{tablenotes}
    \item[\textquotedbl] Same value as cell to left.
    \item[*] Not competition Open maps.
    \end{tablenotes}
    \end{threeparttable}
\end{table}

\begin{table}[H]
    \centering
    \begin{threeparttable}
    \caption{squnet training schedule starting with randomly initialized weights}
    \label{tab:squnet-training-schedule}
    \begin{tabular}{lcccc}
    \toprule
     & Phase 1 & Transition 1→2\tnote{*} & Phase 2 \\
     \midrule
    steps & 100M & 60M & 40M \\
    reward weights\tnote{†} & [0.8, 0.01, 0.19] &  & [0, 0.99, 0.01] \\
    $c_1$ (value loss coef)\tnote{†} & [0.5, 0.1, 0.2] &  & [0, 0.5, 0.1]\\
    $c_2$ (entropy coef) & 0.01 & & 0.001 \\
    learning rate & $10^{-4}$ & & $5 \times 10^{-5}$ \\
    \bottomrule
    \end{tabular}
    \begin{tablenotes}
       \item[*] Values are linearly interpolated between phases based on step count.
       \item[†] Listed weights are for the shaped, win-loss, cost-based values, respectively.
    \end{tablenotes}
    \end{threeparttable}
\end{table}

\section{Behavior cloning details}
\label{appendix:behavior-cloning-details}
\begin{table}[H]
    \centering
    \begin{threeparttable}
    \caption{Neural architecture for behavior cloning and PPO fine-tuned training}
    \label{tab:bc-architecture}
    \begin{tabular}{lc}
    \toprule
                                 & deep16-128 \\
    \midrule
    Levels                      & 3  \\
    Encoder residual blocks/level & [3, 2, 4] \\
    Decoder residual blocks/level & [3, 2] \\
    Stride per level            & [4, 4] \\
    Deconvolution strides per level & [[2, 2], [2, 2]]\tnote{*}\\
    Channels per level          & [128, 128, 128] \\
    Trainable parameters        & 5,027,279 \\
    MACs\tnote{†} (16x16)          & 0.52B \\
    MACs\tnote{†} (64x64)          & 8.40B \\
    \bottomrule
    \end{tabular}
    \begin{tablenotes}
       \item[*] 2 stride-2 transpose convolutions to match the 1 stride-4 convolution.
       \item[†] Multiply-Accumulates for computing actions for a single observation.
    \end{tablenotes}
    \end{threeparttable}
\end{table}


\section{Single player round-robin benchmark setup}
\label{appendix:single-player-benchmark-setup}
In Section~\ref{sec:single-player-benchmark}, \agentName\ plays on the 8 Open maps
against 4 opponents:
\begin{inparaenum}[(1)]
    \item baseline POWorkerRush,
    \item baseline and 2017 competition winner POLightRush,
    \item 2020 competition winner CoacAI, and
    \item last competition (2021) winner Mayari.
\end{inparaenum}
\agentName\ normally plays against each opponent on each map for 100 matches (50
each as player 1 and 2). The exception is squnet (S) on \texttt{BWDistantResources32x32},
where \agentName\ only played each opponent for 20 matches (10 each as player 1 and 2).
All opponents use A* for pathfinding, which is default for competitions.  Win rates are 
percentages of wins where draws count as 0.5 wins for each player. The single player 
round-robin benchmark was run on a 2018 Mac Mini with Intel i7-8700B CPU (6-core, 
3.2GHz) with PyTorch limited to 6 threads. Timeouts were set to 100 ms. If
an agent took 20ms over the deadline (120 ms total), the game was terminated and the win
awarded to the opponent.

In Section~\ref{sec:behavior-cloning-results}, \bcAgent\ and \bcPPOAgent\ play each opponent on each map in 20 only games
(10 each as player 1 and 2).

\section{Additional IEEE-CoG2023 microRTS competition details}
\begin{table}[H]
    \caption{Win rates of all agents in the IEEE-CoG 2023 microRTS competition on Open maps.  Player 1 is the row agent and player 2 is the column agent. Each win rate value is the percentage of games won by player 1 (the row agent).}
    \label{tab:all-competition-winrate}
    \begin{center}
    \begin{tabular}{lccccccccccccccccc|c}
    & \begin{sideways} Mayari \end{sideways} & \begin{sideways} 2L \end{sideways} 
    & \begin{sideways} \textbf{\agentName} \end{sideways} & \begin{sideways} ObiBotKenobi \end{sideways} 
    & \begin{sideways} Aggrobot \end{sideways} & \begin{sideways} sophia \end{sideways} 
    & \begin{sideways} bRHEAdBot \end{sideways} & \begin{sideways} Ragnar \end{sideways} 
    & \begin{sideways} POLightRush \end{sideways} & \begin{sideways} SaveTheBeesV4 \end{sideways} 
    & \begin{sideways} POWorkerRush \end{sideways} & \begin{sideways} MyMicroRtsBot \end{sideways} 
    & \begin{sideways} NaiveMCTS \end{sideways} & \begin{sideways} myBot \end{sideways} 
    & \begin{sideways} NIlSiBot \end{sideways} & \begin{sideways} Predator \end{sideways} 
    & \begin{sideways} RandomBiasedAI \end{sideways} & \begin{sideways} Overall \end{sideways} \\
    \midrule
    Mayari         & -      & 53 & 32          & 73           & 78       & 93     & 95        & 64     & 88          & 93            & 75           & 78            & 100       & 100   & 100      & 100      & 100            & 82      \\
    2L             & 51     & -  & 39          & 50           & 69       & 63     & 93        & 56     & 75          & 98            & 88           & 81            & 76        & 94    & 94       & 95       & 96             & 76      \\
    \textbf{\agentName}    & 62     & 59 & -           & 49           & 64       & 71     & 64        & 64     & 64          & 78            & 78           & 76            & 84        & 94    & 73       & 87       & 87             & 72      \\
    ObiBotKenobi   & 39     & 29 & 47          & -            & 47       & 69     & 60        & 56     & 58          & 83            & 65           & 76            & 72        & 99    & 79       & 85       & 100            & 66      \\
    Aggrobot       & 9      & 25 & 26          & 60           & -        & 69     & 55        & 44     & 63          & 86            & 69           & 94            & 66        & 94    & 94       & 91       & 94             & 65      \\
    sophia         & 25     & 44 & 30          & 35           & 38       & -      & 41        & 88     & 75          & 76            & 63           & 69            & 71        & 100   & 75       & 84       & 83             & 62      \\
    bRHEAdBot      & 4      & 7  & 24          & 44           & 49       & 69     & -         & 51     & 64          & 79            & 59           & 65            & 83        & 99    & 81       & 96       & 98             & 61      \\
    Ragnar         & 40     & 50 & 32          & 26           & 50       & 13     & 46        & -      & 44          & 71            & 63           & 69            & 73        & 88    & 81       & 73       & 85             & 56      \\
    POLightRush    & 0      & 25 & 29          & 38           & 31       & 44     & 34        & 38     & -           & 71            & 69           & 69            & 73        & 100   & 75       & 91       & 100            & 55      \\
    SaveTheBeesV4  & 14     & 9  & 21          & 43           & 31       & 59     & 38        & 47     & 66          & -             & 50           & 57            & 81        & 86    & 85       & 90       & 93             & 54      \\
    POWorkerRush   & 13     & 13 & 21          & 29           & 31       & 44     & 44        & 56     & 38          & 89            & -            & 75            & 49        & 94    & 81       & 81       & 96             & 53      \\
    MyMicroRtsBot  & 11     & 13 & 15          & 25           & 38       & 56     & 38        & 56     & 38          & 86            & 44           & -             & 43        & 94    & 69       & 74       & 92             & 49      \\
    NaiveMCTS      & 0      & 11 & 17          & 22           & 34       & 27     & 15        & 26     & 29          & 69            & 56           & 58            & -         & 92    & 46       & 60       & 84             & 40      \\
    myBot          & 1      & 6  & 21          & 20           & 39       & 48     & 28        & 41     & 43          & 77            & 39           & 40            & 50        & -     & 55       & 66       & 66             & 40      \\
    NIlSiBot       & 0      & 13 & 18          & 18           & 31       & 25     & 13        & 13     & 31          & 63            & 31           & 38            & 51        & 81    & -        & 58       & 73             & 35      \\
    Predator       & 1      & 7  & 13          & 6            & 12       & 21     & 11        & 16     & 14          & 56            & 22           & 28            & 44        & 73    & 43       & -        & 45             & 26      \\
    RandomBiasedAI & 0      & 1  & 15          & 0            & 4        & 15     & 6
    & 9      & 4           & 52            & 4            & 13            & 18        &
    85    & 39       & 39       & - & 19 \\
    \end{tabular}
\end{center}
\end{table}

\begin{table}[H]
    \caption{\agentName\ win rates split by competition job. Each map has 5 or 10 jobs that runs a round robin tournament of all agents for 2 or 1 iterations, respectively. "Outlier" jobs are bolded. "Average" is the average win rate for all jobs. "Average Removing Outliers" is the average win rate for all jobs excluding "outlier" jobs.}
    \label{tab:outlier-winrate}
    \begin{center}
    \begin{tabular}{r|rrrrrrrr|l}
    & \begin{sideways}\texttt{basesWorkers8x8A}\end{sideways} &
    \begin{sideways}\texttt{FourBasesWorkers8x8}\end{sideways} &
    \begin{sideways}\texttt{NoWhereToRun9x8}\end{sideways} &
    \begin{sideways}\texttt{basesWorkers16x16A}\end{sideways} &
    \begin{sideways}\texttt{TwoBasesBarracks16x16}\end{sideways} &
    \begin{sideways}\texttt{DoubleGame24x24}\end{sideways} &
    \begin{sideways}\texttt{BWDistantResources32x32}\end{sideways} &
    \begin{sideways}\texttt{(4)BloodBath.scmB}\end{sideways} & 
    \begin{sideways}Overall\end{sideways}\\
    \midrule
    & 98 & 97 & 97 & 100 & 94 & 99 & 49 & 34 &  \\
    & 100 & 94 & 97 & 100 & \textbf{4} & 96 & 53 & 38 &  \\
    & 100 & 95 & \textbf{33} & 100 & 92 & 100 & 54 & 38 &  \\
    & \textbf{5} & 92 & 94 & 98 & 94 & 97 & 58 & 39 &  \\
    & \textbf{28} & 95 & 100 & 100 & 94 & \textbf{9} & 58 & \textbf{19} &  \\
    & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & 100 & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & 39 &  \\
    & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & 97 & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \textbf{19} &  \\
    & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \textbf{30} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & 44 &  \\
    & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & 97 & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & 36 &  \\
    & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & 95 & \multicolumn{1}{l}{} &
    \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & 36 &  \\
    \hline
   Average & 66 & 95 & 84 & 100 & 75 & 80 & 54 & 34 & \multicolumn{1}{r}{74} \\
    Average Removing Outliers & 99 & 95 & 97 & 100 & 93 & 98 & 54 & 38 & \multicolumn{1}{r}{84} \\
    \end{tabular}
\end{center}
\end{table}

\section{Additional behavior cloning benchmarks}
\begin{table}[H]
    \caption{\bcAgent\ win rate in single player round-robin benchmark. Win rates above 50 are bolded.}
    \label{tab:bc-winrate}
    \begin{center}
    \begin{tabular}{lcccc|c}
     & POWorkerRush & POLightRush & CoacAI & Mayari & Overall \\
    \midrule
    \texttt{basesWorkers8x8A} & \textbf{60} & \textbf{100} & \textbf{90} & 50 & 75 \\
    \texttt{FourBasesWorkers8x8} & \textbf{100} & \textbf{100} & \textbf{85} & \textbf{65} & 88 \\
    \texttt{NoWhereToRun9x8} & \textbf{100} & \textbf{100} & \textbf{83} & \textbf{55} & 85 \\
    \texttt{basesWorkers16x16A} & 10 & \textbf{100} & \textbf{100} & 28 & 60 \\
    \texttt{TwoBasesBarracks16x16} & \textbf{100} & \textbf{100} & 43 & 20 & 66 \\
    \texttt{DoubleGame24x24} & 0 & \textbf{100} & \textbf{100} & 30 & 58 \\
    \texttt{BWDistantResources32x32} & 48 & \textbf{100} & \textbf{100} & \textbf{65} & 78 \\
    \texttt{(4)BloodBath.scmB} & \textbf{100} & \textbf{63} & 20 & 40 & 56 \\
    \hline
    AI Average & \textbf{65} & \textbf{96} & \textbf{78} & 44 & 71 \\
    \end{tabular}
    \end{center}
\end{table}
\end{document}