\documentclass{article}
\usepackage{iclr2024_conference,times}
\begin{document}
\title{A Competition Winning Deep Reinforcement Learning Agent in microRTS}
\author{Scott Goodfriend$^{1}$ \\
$^{1}$ goodfriend.scott@gmail.com \\
}
\maketitle
\begin{abstract}
    Scripted solutions with pathfinding algorithms have predominantly won the five
    previous iterations of the IEEE microRTS ($\mu$RTS) competitions hosted at CIG and
    CoG. Despite Deep Reinforcement Learning (DRL) algorithms making significant strides
    in RTS-style games, their adoption has been limited in this primarily academic
    competition due to the considerable training resources required and the complexity
    inherent in creating and debugging such agents. The RAISocketAI, as part of the
    IEEE-CoG2023 microRTS competition, has emerged as the first winning DRL agent. In a
    non-performance constrained environment, RAISocketAI regularly defeated the two
    preceding competition winners. This first competition-winning DRL submission can be
    a benchmark for future microRTS competitions and a starting point for future DRL
    research. In particular, this submission successfully used fine-tuning a base model
    to perform significantly better on specific maps. Further work in Behavior Cloning
    has proven promising as an even more economical way to bootstrap a model.
\end{abstract}
\section{Introduction}
Deep reinforcement learning has proven to be a powerful tool for solving complex
problems requiring several steps to achieve a goal, such as Atari games \citep{DBLP:journals/corr/MnihKSGAWR13}, continuous
control tasks \citep{DBLP:journals/corr/LillicrapHPHETS15}, and even real-time strategy
(RTS) games. AlphaStar is a famous example of an agent trained with DRL on StarCraft
II to defeat professional players \citep{Vinyals2019GrandmasterLI}. However, AlphaStar was trained with thousands of
CPUs and GPUs/TPUs for several weeks. RTS games are particularly challenging for DRL for
several reasons:
\begin{enumerate}
    \item the state and action spaces are large and varied with different terrain and
        unit types;
    \item each unit type can have different actions and abilities;
    \item each action can control several units;
    \item rewards are sparse (win, loss, or tie) and delayed by possibly several
    thousand timesteps;
    \item winning requires combining tactical (micro) and strategic (macro) decisions;
    \item actions must be taken in real-time (i.e., the game won't wait for the agent to
        take an action);
    \item the agent might not have full visibility of the game state (i.e., fog of war); and
    \item events in the game might be non-deterministic.
\end{enumerate}

microRTS (stylized as $\mu$RTS) is a minimalist, open-source, two-player, zero-sum RTS game testbed designed for research
purposes \citep{Ontan2013TheCM}. It includes many aspects of RTS games, simplified: different unit types, unit-specific
actions terrain, resource collection and consuptiom to build units, and unit-to-unit combat
where units have different strengths and weaknesses. microRTS also supports fog of war
and non-determinism; however, these were disabled for the IEEE-CoG2023 microRTS
competition.

The IEEE microRTS competition has been hosted at the Conference on Games (CoG) nearly
every year since 2019 and at the Conference on Computational Intelligence and Games
(CIG) before that since 2017 \citep{Ontañón_Barriga_Silva_Moraes_Lelis_2018}.
Competitors submit an agent that plays against other agents in a round-robin tournament
on 12 maps with different distributions of terrain, resources, and starting units and
buildings. 8 "Open" maps are known beforehand and are used to determine the competition winner.
The other 4 "Hidden" maps are not revealed to participants until after the competition.
These hidden maps are meant to test the generalization ability of the agents, but aren't
part of the competition to allow organizers to participate. Hidden map results are also
released.

This paper describes the RAISocketAI agent, which won the IEEE-CoG2023 microRTS
competition and is the first DRL agent to win a microRTS competition. RAISocketAI
extends the work of MicroRTS-Py (formally Gym-$\mu$RTS/Gym-MicroRTS) to be competitive
in the Open competition maps. MicroRTS-Py is an OpenAI Gym environment for microRTS,
which makes it easier to train DRL agents \citep{DBLP:journals/corr/abs-2105-13807}.
MicroRTS-Py also includes a DRL training framework using Proximal Policy Optimization
(PPO) \citep{DBLP:journals/corr/SchulmanWDRK17}, state and action space vectorization,
invalid action masking, and environment vectorization.
\citet{DBLP:journals/corr/abs-2105-13807} only trained an agent for one of the smaller
maps. RAISocketAI extends the DRL training framework to be competitive on other, more
complicated maps.

Models in the IEEE microRTS competition are supposed to submit actions for each timestep
within 100ms. Without GPU acceleration, this is a significant constraint for deep neural
network agents. Being a Python agent, RAISocketAI also had to communicate with the Java
microRTS efficiently, which required optimizing the data representation.

Despite these constraints, RAISocketAI won the IEEE-CoG2023 microRTS competition. The
agent consists of 7 trained neural networks. It took about 70 GPU-days to train these
models, which is within budget for researchers, though possibly outside of budget for
students. However, 4 of these neural networks were trained from an existing model. These
transfer learning models were used to make the models perform better on specific maps.
These transfer learning models took in total 18 GPU-days to train.

While microRTS doesn't support human players, the IEEE microRTS competition means there
are several agents available to use imitation learning on. Work following the
competition shows that behavior cloning and fine-tuning with PPO can be used to train a
competitive agent more economically. Using the same playthroughs to train the critic
heads on win-loss rewards means that PPO can be trained with just sparse win-loss
rewards, eliminating the need for a difficult to tune human reward function.

\section{Related Work}
\subsection{MicroRTS-Py}
MicroRTS-Py\footnote{https://github.com/Farama-Foundation/MicroRTS-Py} is an OpenAI Gym
wrapper for microRTS and includes a Proximal Policy Optimization (PPO) implementation
trained on 1 of the Open maps (\texttt{16x16basesWorkers}). Starting from a PPO
implementation matching \textit{openai/baselines},
\citet{DBLP:journals/corr/abs-2105-13807} added action composition, a shaped reward function, invalid action
masking, IMPALA-CNN (a convolutional neural network with residual blocks), and trained
against a diverse set of scripted agents. The agent was trained on a single map for 300
million game steps, which took less than 3 GPU-days. The agent achieved a
91\% win rate against a diverse set of competition bots.

\citet{DBLP:journals/corr/abs-2105-13807} ran ablation studies across their additions.
They found invalid action masking was essential to have an agent that could compete at
the most basic level (82\% win rate with invalid action masking, 0\% without). Using the
residual block network IMPALA-CNN architecture instead of the network used by
\citet{DBLP:journals/corr/MnihKSGAWR13} got the win rate up the rest of the way.

They also tried two different ways to issue player actions: Unit Action Simulation (UAS)
and GridNet \citep{pmlr-v97-han19a}. UAS calls the policy iteratively on each unit, simulating the game state
after each unit action before issuing all unit actions combined to the game. GridNet
computes the actions for all units in a single policy call by computing unit action
logits for all grid positions and using a player action mask to ignore cells that don't
have any units owned by the player. UAS performed better than GridNet (91\% vs 89\%).
Despite UAS's better performance, MicroRTS-Py is deprecating UAS in favor of GridNet
because UAS's more complex implementation and difficulty to incorporate selfplay and
imitation learning, both features important in the current and future versions of
RAISocketAI.

\citet{DBLP:journals/corr/abs-2105-13807} also tried selfplay, training the policy in
games where the policy played itself. They found that selfplay didn't improve win rate,
either when only using selfplay or when training with half selfplay and half scripted
bots. The other policies were trained as only the first player while selfplay required
the policy train as both players, so the comparison might not be fair.

RAISocketAI reimplements must of MicroRTS-Py, including the PPO implementation, action
composition, shaped reward function, invalid action masking, GridNet, selfplay, and
scripted bot training. RAISocketAI extended the observation space to support walls and
made the invalid action mask more strict. RAISocketAI and MicroRTS-Py were trained with partial
observability and non-determinism disabled.

\subsection{DeepMind AlphaStar}
\citet{Vinyals2019GrandmasterLI}'s AlphaStar is a grandmaster-level AI trained with DRL to play the RTS game
StarCraft II. AlphaStar was trained with a combination of supervised learning using
games sampled from a dataset of human replays and reinforcement learning using a
league-based system to train multiple agents against each other. The supervised agent
was rated in the top 16\% of human players and was used as starting points for
reinforcement learning. Reinforcement learning is used to train agents, which are
checkpointed to a league intermittently. The training framework trains 3 types of agents
for each of the 3 in-game races: a main agent, agents trained to defeat the main agent, and agents
trained against the league to find exploits in those policies trained so far.

AlphaStar used a custom, but similar interface to the StarCraft II Learning Environment
(PySC2). Observations are a list of units with attributes visible to the agent. Enemy
units outside of the visiblity of agent's units are not included in the observation.
Enemy units outside of the agent's camera view have certain attributes hidden. Agent
actions are similar to the actions available to human players, which differs from
microRTS which expects per game step actions to each unit.

AlphaStar was trained on 3072 TPU cores and 50,400 preemptible CPU cores for a duration of 44 days. The
league-sytem to enforce a diversity of strategies meant several policies were being
trained at once. Additionally, StarCraft II itself is a more expensive game to
simulate, requiring extensive resources to run games in parallel.

microRTS is a much simpler game where several instances of the game can easily be run
simultaneously. On a single GPU computer, RAISocketAI trained at over 360 game
steps/second. While RAISocketAI didn't use supervised learning to bootstrap the agent,
following work has used behavior cloning to train a competitive agent in a fraction of
the time of the from-scratch trained RAISocketAI.

\subsection{Lux AI Kaggle competitions}
Kaggle has hosted many Simulations competitions. In Simulation competitions, competitors
submit agents that play against other submitted agents in some turn-based game
environment. The first Lux AI competition was the first major Kaggle competition that a
DRL agent won \citep{lux-ai-2021}. Lux is a resource management game where agents
control worker and city units to gather resources, build more units, and control the
board. Similar to microRTS, Lux is represented as a grid of cells with units, cities,
and resources occupying cells. The winning agent by \citet{lux-ai-2021-winner} used a
similar grid representation for the observation, GridNet action space, and reward
shaping. The agent was trained with IMPALA with UPGO and TD-lambda loss terms. Instead
of training with a shaped reward function through the entirety of training, \citet{lux-ai-2021-winner}
used shaped rewards on a smaller map before transitioning to sparse win-loss rewards on
larger and competition-size maps.

The top DRL agent in the Lux AI Season 2 competition placed 4th overall, beaten out by 3
human scripted agents. \citet{Ferdinand2021doublecone} used a "DoubleCone" neural
network backbone with critic and actor heads. Similar to ResNet with a series of
residual blocks, DoubleCone ran the middle residual blocks of the network downscaled 4x
before upscaling back up to full resolution, adding a residual connection from the
beginning of downscaling to the end of upscaling. The submission agent used
DoubleCone(4, 6, 4): 4 residual blocks from the input, 6 blocks downscaled, followed by
4 resdiaul blocks at full resolution. This downscaling was a performance optimization at
inference time. Similar to the first season's winner, \citet{Ferdinand2021doublecone}
used a shaped reward in a setup phase before switching to sparse win-loss rewards. MicroRTS uses the DoubleCone architecture and switching from shaped to sparse rewards
during training.

\section{Methods}


\bibliography{iclr2024}
\bibliographystyle{iclr2024_conference}
\end{document}