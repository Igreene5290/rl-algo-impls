\documentclass{article}
\usepackage{iclr2024_conference}
\input{math_commands.tex}
\usepackage{hyperref}
\usepackage{url}
\usepackage{times}
\usepackage{graphicx}
\usepackage{array}
\usepackage{paralist}
\usepackage{float}
\usepackage[utf8]{inputenc}
\usepackage{threeparttable}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage{tabularx}
\usepackage{ragged2e}
\usepackage{adjustbox}
\usepackage{rotating}

\title{A Competition Winning Deep Reinforcement Learning Agent in microRTS}

\author{Scott Goodfriend \\
\texttt{goodfriend.scott@gmail.com} \\
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

% \iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\usepackage{iclr2024_anonymization}

\begin{document}

\maketitle
\begin{abstract}
    Scripted solutions with pathfinding algorithms have predominantly won the five
    previous iterations of the IEEE microRTS ($\mu$RTS) competitions hosted at CIG and
    CoG. Despite Deep Reinforcement Learning (DRL) algorithms making significant strides
    in RTS-style games, their adoption has been limited in this primarily academic
    competition due to the considerable training resources required and the complexity
    inherent in creating and debugging such agents. \agentName\ is the first DRL agent
    to win the IEEE microRTS competition. In a
    non-performance constrained environment, \agentName\ regularly defeated the two
    preceding competition winners. This first competition-winning DRL submission can be
    a benchmark for future microRTS competitions and a starting point for future DRL
    research. In particular, this submission successfully used fine-tuning a base model
    to perform significantly better on specific maps. Further work in Behavior Cloning
    has proven promising as an even more economical way to bootstrap a model.
\end{abstract}
\section{Introduction}
Deep reinforcement learning has proven to be a powerful tool for solving complex
problems requiring several steps to achieve a goal, such as Atari games \citep{DBLP:journals/corr/MnihKSGAWR13}, continuous
control tasks \citep{DBLP:journals/corr/LillicrapHPHETS15}, and even real-time strategy
(RTS) games. AlphaStar is a famous example of an agent trained with DRL on StarCraft
II to defeat professional players \citep{Vinyals2019GrandmasterLI}. However, AlphaStar was trained with thousands of
CPUs and GPUs/TPUs for several weeks. RTS games are particularly challenging for DRL for
several reasons:
\begin{inparaenum}[(1)]
    \item the state and action spaces are large and varied with different terrain and
        unit types;
    \item each unit type can have different actions and abilities;
    \item each action can control several units;
    \item rewards are sparse (win, loss, or tie) and delayed by possibly several
    thousand timesteps;
    \item winning requires combining tactical (micro) and strategic (macro) decisions;
    \item actions must be taken in real-time (i.e., the game won't wait for the agent to
        take an action);
    \item the agent might not have full visibility of the game state (i.e., fog of war); and
    \item events in the game might be non-deterministic.
\end{inparaenum}

microRTS (stylized as $\mu$RTS) is a minimalist, open-source, two-player, zero-sum RTS game testbed designed for research
purposes \citep{Ontan2013TheCM}. It includes many aspects of RTS games, simplified: different unit types, unit-specific
actions terrain, resource collection and consuptiom to build units, and unit-to-unit combat
where units have different strengths and weaknesses. microRTS also supports fog of war
and non-determinism; however, these were disabled for the IEEE-CoG2023 microRTS
competition.

The IEEE microRTS competition has been hosted at the Conference on Games (CoG) nearly
every year since 2019 and at the Conference on Computational Intelligence and Games
(CIG) before that since 2017 \citep{Ontañón_Barriga_Silva_Moraes_Lelis_2018}.
Competitors submit an agent that plays against other agents in a round-robin tournament
on 12 maps with different distributions of terrain, resources, and starting units and
buildings. 8 "Open" maps are known beforehand and are used to determine the competition winner.
The other 4 "Hidden" maps are not revealed to participants until after the competition.
These hidden maps are meant to test the generalization ability of the agents, but aren't
part of the competition to allow organizers to participate. Hidden map results are also
released.

This paper describes the \agentName\ agent\footnote{\raiMicroRTSGitHubUrl}, which won the IEEE-CoG2023 microRTS
competition and is the first DRL agent to win a microRTS competition. \agentName\
extends the work of MicroRTS-Py (formally Gym-$\mu$RTS/Gym-MicroRTS) to be competitive
in the Open competition maps. MicroRTS-Py is an OpenAI Gym environment for microRTS,
which makes it easier to train DRL agents \citep{DBLP:journals/corr/abs-2105-13807}.
MicroRTS-Py also includes a DRL training framework using Proximal Policy Optimization
(PPO) \citep{DBLP:journals/corr/SchulmanWDRK17}, state and action space vectorization,
invalid action masking, and environment vectorization.
\citet{DBLP:journals/corr/abs-2105-13807} only trained an agent for one of the smaller
maps. \agentName\ extends the DRL training framework to be competitive on other, more
complicated maps.

Models in the IEEE microRTS competition are supposed to submit actions for each timestep
within 100ms. Without GPU acceleration, this is a significant constraint for deep neural
network agents. Being a Python agent, \agentName\ also had to communicate with the Java
microRTS efficiently, which required optimizing the data representation.

Despite these constraints, \agentName\ won the IEEE-CoG2023 microRTS competition. The
agent consists of 7 trained neural networks. It took about 70 GPU-days to train these
models, which is within budget for researchers, though possibly outside of budget for
students. However, 4 of these neural networks were trained from an existing model. These
transfer learning models were used to make the models perform better on specific maps.
These transfer learning models took in total 18 GPU-days to train.

While microRTS doesn't support human players, the IEEE microRTS competition means there
are several agents available to use imitation learning on. Work following the
competition shows that behavior cloning and fine-tuning with PPO can be used to train a
competitive agent more economically. Using the same playthroughs to train the critic
heads on win-loss rewards means that PPO can be trained with just sparse win-loss
rewards, eliminating the need for a difficult to tune human reward function.

\section{Related Work}
\subsection{MicroRTS-Py}
MicroRTS-Py\footnote{\url{https://github.com/Farama-Foundation/MicroRTS-Py}} is an OpenAI Gym
wrapper for microRTS and includes a Proximal Policy Optimization (PPO) implementation
trained on 1 of the Open maps (\texttt{16x16basesWorkers}). Starting from a PPO
implementation matching \textit{openai/baselines},
\citet{DBLP:journals/corr/abs-2105-13807} added action composition, a shaped reward function, invalid action
masking, IMPALA-CNN (a convolutional neural network with residual blocks), and trained
against a diverse set of scripted agents. The agent was trained on a single map for 300
million game steps, which took less than 3 GPU-days. The agent achieved a
91\% win rate against a diverse set of competition bots.

\citet{DBLP:journals/corr/abs-2105-13807} ran ablation studies across their additions.
They found invalid action masking was essential to have an agent that could compete at
the most basic level (82\% win rate with invalid action masking, 0\% without). Using the
residual block network IMPALA-CNN architecture instead of the network used by
\citet{DBLP:journals/corr/MnihKSGAWR13} got the win rate up the rest of the way.

They also tried two different ways to issue player actions: Unit Action Simulation (UAS)
and GridNet \citep{pmlr-v97-han19a}. UAS calls the policy iteratively on each unit, simulating the game state
after each unit action before issuing all unit actions combined to the game. GridNet
computes the actions for all units in a single policy call by computing unit action
logits for all grid positions and using a player action mask to ignore cells that don't
have any units owned by the player. UAS performed better than GridNet (91\% vs 89\%).
Despite UAS's better performance, MicroRTS-Py is deprecating UAS in favor of GridNet
because UAS's more complex implementation and difficulty to incorporate selfplay and
imitation learning, both features important in the current and future versions of
\agentName.

\citet{DBLP:journals/corr/abs-2105-13807} also tried selfplay, training the policy in
games where the policy played itself. They found that selfplay didn't improve win rate,
either when only using selfplay or when training with half selfplay and half scripted
bots. The other policies were trained as only the first player while selfplay required
the policy train as both players, so the comparison might not be fair.

\agentName\ reimplements must of MicroRTS-Py, including the PPO implementation, action
composition, shaped reward function, invalid action masking, GridNet, selfplay, and
scripted bot training. \agentName\ extended the observation space to support walls and
made the invalid action mask more strict. \agentName\ and MicroRTS-Py were trained with partial
observability and non-determinism disabled.

\subsection{DeepMind AlphaStar}
\citet{Vinyals2019GrandmasterLI}'s AlphaStar is a grandmaster-level AI trained with DRL to play the RTS game
StarCraft II. AlphaStar was trained with a combination of supervised learning using
games sampled from a dataset of human replays and reinforcement learning using a
league-based system to train multiple agents against each other. The supervised agent
was rated in the top 16\% of human players and was used as starting points for
reinforcement learning. Reinforcement learning is used to train agents, which are
checkpointed to a league intermittently. The training framework trains 3 types of agents
for each of the 3 in-game races: a main agent, agents trained to defeat the main agent, and agents
trained against the league to find exploits in those policies trained so far.

AlphaStar used a custom, but similar interface to the StarCraft II Learning Environment
(PySC2). Observations are a list of units with attributes visible to the agent. Enemy
units outside of the visiblity of agent's units are not included in the observation.
Enemy units outside of the agent's camera view have certain attributes hidden. Agent
actions are similar to the actions available to human players, which differs from
microRTS which expects per game step actions to each unit.

AlphaStar was trained on 3072 TPU cores and 50,400 preemptible CPU cores for a duration of 44 days. The
league-sytem to enforce a diversity of strategies meant several policies were being
trained at once. Additionally, StarCraft II itself is a more expensive game to
simulate, requiring extensive resources to run games in parallel.

microRTS is a much simpler game where several instances of the game can easily be run
simultaneously. On a single GPU computer, \agentName\ trained at over 360 game
steps/second. While \agentName\ didn't use supervised learning to bootstrap the agent,
following work has used behavior cloning to train a competitive agent in a fraction of
the time of the from-scratch trained \agentName.

\subsection{Lux AI Kaggle competitions}
Kaggle has hosted many Simulations competitions. In Simulation competitions, competitors
submit agents that play against other submitted agents in some turn-based game
environment. The first Lux AI competition was the first major Kaggle competition that a
DRL agent won \citep{lux-ai-2021}. Lux is a resource management game where agents
control worker and city units to gather resources, build more units, and control the
board. Similar to microRTS, Lux is represented as a grid of cells with units, cities,
and resources occupying cells. The winning agent by \citet{lux-ai-2021-winner} used a
similar grid representation for the observation, GridNet action space, and reward
shaping. The agent was trained with IMPALA with UPGO and TD-lambda loss terms. Instead
of training with a shaped reward function through the entirety of training, \citet{lux-ai-2021-winner}
used shaped rewards on a smaller map before transitioning to sparse win-loss rewards on
larger and competition-size maps.

The top DRL agent in the Lux AI Season 2 competition placed 4th overall, beaten out by 3
human scripted agents. \citet{Ferdinand2021doublecone} used a "DoubleCone" neural
network backbone with critic and actor heads. Similar to ResNet with a series of
residual blocks, DoubleCone ran the middle residual blocks of the network downscaled 4x
before upscaling back up to full resolution, adding a residual connection from the
beginning of downscaling to the end of upscaling. The submission agent used
DoubleCone(4, 6, 4): 4 residual blocks from the input, 6 blocks downscaled, followed by
4 resdiaul blocks at full resolution. This downscaling was a performance optimization at
inference time. Similar to the first season's winner, \citet{Ferdinand2021doublecone}
used a shaped reward in a setup phase before switching to sparse win-loss rewards. MicroRTS uses the DoubleCone architecture and switching from shaped to sparse rewards
during training.

\section{Methods}
\subsection{\agentName}
To participate in the competition, \agentName\ has a Java class that handles turn
handling and resetting commands from the Java competition code. This Java class starts a
Python process and game state data is passed between the Java and Python processes using
binary data over a pipe. This differs from the suggested solution to use a socket and
JSON or XML data as a performance optimization for the larger maps.

The submission version of the Python process loads 7 different policy networks, only one network is used
for a given map (Table~\ref{tab:policy-networks}). Networks are chosen by prioritizing
the compatible networks and then picking the highest priority network that is capable of
running within the allotted time on the current hardware. Map-specific networks are
prioritized higher than size-specific networks. Multiple networks can be prioritized for
the same map or map size. While microRTS supports any map size (even non-square), policy networks were trained on
powers of 2 map sizes starting from 16x16 (i.e., 16x16, 32x32, 64x64). When running an
observation through a policy network, the observation is padded to the next power of 2.

\subsection{Neural network architecture}
\agentName\ uses two different neural network architectures: DoubleCone(4, 6, 4) and a smaller, deeper network
we call squnet. DoubleCone(4, 6, 4) consists of
\begin{inparaenum}[(1)]
    \item 4 residual blocks;
    \item a downscaled residual block consisting of a stride-4 convolution, 6 residual blocks, and
        2 stride-2 transpose convolutions;
    \item 4 residual blocks; and
    \item actor and value heads (Figure~\ref{fig:doublecone}).
\end{inparaenum}
Each residual block includes a squeeze-excitation layer after the second convoluational
layer (Figure~\ref{fig:squeezeexcitation}). The actor head is a convoluational layer
that outputs logits for unit actions at every position. A unit action is composed of
independent discrete subactions: $D = \{a_{action\ type},$ $a_{move\ direction},$ $a_{harvest\
direction},$ $a_{return\ direction},$ $a_{produce\ direction},$ $a_{produce\ type},$ $a_{relative\
attack\ position}\}$. This amounts to 78 logits per position. Every position Invalid action masking sets
logits to a very large negative number (thus zeroing probabilities and gradients) for actions that are
illegal or would accomplish nothing (e.g., moving a unit to an occupied or reserved
position) \citep{DBLP:journals/corr/abs-2006-14171}. This masking significantly reduces the action space per turn, thus making
training more efficient. The values heads are each 
\begin{inparaenum}[(1)]
    \item 2 stride-2 convolutions,
    \item an adaptive average pooling layer,
    \item flattened,
    \item 2 densely connected layers, and
    \item an activation function (Identity [no] or Tanh) to a single, scalar value (Figure~\ref{fig:valueheads}).
\end{inparaenum}
The adaptive average pooling layer allows the network to be used on different map sizes.

While DoubleCone could support any map size, inference would likely exceed 100
milliseconds for larger maps. Therefore, for larger maps, we used a different
architecture, squnet, which nests 3 downscaling blocks (Figure~\ref{fig:squnet}). This 
creates a network shaped like U-Net; however, the network is much more similar to DoubleCone than U-Net.
This aggressive downscaling reduces the number of operations necessary during inference,
especially for larger maps (Table~\ref{table:architectureBreakdown}).

Instead of 1 value head, \agentName\ uses 3 values heads for 3 different value
functions:
\begin{inparaenum}[(1)]
    \item dense shaped reward similar to MicroRTS-Py except each combat unit type is
    scaled by build-time (thus rewarding more expensive units),
    \item win-loss sparse reward at game end (Tanh activation), and
    \item in-game difference in units based on cost (similar to the reward function used
    by \citet{Winter2021}).
\end{inparaenum}
These 3 value heads are used to mix-and-match rewards over the course of training,
generally starting with dense rewards using heads (1) and (3) and finishing with only
win-loss sparase rewards by the end.

\subsection{Initial Training}
\agentName\ reimplements much of MicroRTS-Py, including the PPO implementation, action
composition, shaped reward function, invalid action masking, GridNet, selfplay, and
scripted bot training. \agentName's codebase\footnote{\rlAlgoImplsGitHubUrl} supports 
multiple environments and reimplementation allowed the environment to fit into the 
existing codebase. The observation representation was extended in two ways: (1) walls 
were added to the observation as two boolean flags per position and (2) added the 
destination of units as invalid move targets in the invalid action mask (only the unit's current location was considered
invalid in MicroRTS-Py; however, the microRTS environment doesn't allow units to move into
where another unit is moving into). Padding of maps was handled by making all padding
positions into walls. We also fixed a bug where resources (which are unowned) were being counted as owned by
the opponent if the agent was the second player\footnote{\unownedFixGitHubCommit}.

Training uses the PPO loss function from
\citet{DBLP:journals/corr/SchulmanWDRK17}:
\begin{align}
    L^{CLIP+VF+S}(\theta) &= \hat{\mathbb{E}}_t \left[ L^{CLIP}(\theta) - c_1 L^{VF}(\theta) + c_2 S\left[\pi_\theta\right](s_t) \right], \\
    L^{CLIP}(\theta) &= \min \left( \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)} \hat{A}_t, \text{clip}\left( \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}, 1-\epsilon, 1+\epsilon \right) \hat{A}_t \right), \\
    L^{VF}(\theta) &= \frac{1}{2} \left( V_\theta(s_t) - \hat{V}_t \right)^2,
\end{align}
where $c_1$ is the value loss coefficient, $c_2$ is the entropy coefficient, $S$ is an
entropy bonus function, $\pi_\theta$ is the stochastic policy, $\pi_{\theta_{old}}$ is the rollout policy, $a_t$
is the action taken at time $t$, $s_t$ is the observation at time $t$, $\hat{A}_t$
is the advantage estimate,  $\epsilon$ is the clipping coefficient, $V_\theta$ is the
value function, $\hat{V}_t$ is the return estimate. The advantage estimate is computed
using Generalized Advantage Estimation (GAE) \citep{DBLP:journals/corr/SchulmanMLJA15}.
We compute a separate advantage for each of the 3 rewards with each advantage estimate
using independent $\gamma$ (discount factor) and $\lambda$ (exponetial weight discount)
values. The 3 advantages are weighted summed together to get the final advantage
estimate.

Over the course of training, the reward weights, value loss coefficients (each value
head has its own loss coefficient), entropy coefficient, and learning rate are varied on
a schedule. At the start of training, the policy loss heavily weights the dense, shaped 
reward advantage, and the value loss similar weights the dense shaped value head. Over
the course of training, both losses are weighted more towards the win-loss sparse and
eventually take no contribution from the shaped reward. The entropy coefficient is also
lowered at the end of training to discourage the agent from making random moves as the
learning rate is also lowered.

We first trained \agentName\ with selfplay both against itself and against prior
versions of itself. We reached a 91\% win rate against the same bots MicroRTS-Py was
benchmarked against. However, it only beat CoacAI (the 2020 competition winner) in 20\%
of games. The best performing agent of MicroRTS-Py nearly always beat CoacAI, though
the best versions of GridNet also usually lost against CoacAI. Therefore, we fine-tuned
the model through 3 iterations:
\begin{inparaenum}[(1)]
    \item one-half of environments trained against CoacAI, replacing prior
    versions of itself;
    \item one-half of environments trained against CoacAI or Mayari (2021 competition
    winner) split evenly and primarily trained on win-loss rewards; and
    \item same as (2) but included action mask improvements and added a GELU activation
    after the stride-4 convolution to match \citet{Ferdinand2021doublecone}'s DoubleCone.
\end{inparaenum}
By the end of fine-tuning, the model was winning 98\% of games, including about 90\%
against each of CoacAI and Mayari.

\subsection{Transfer learning}
The model so far had been trained on the 5 smaller competition Open maps. Using the
fine-tuned model as a starting pint, we trained 3 additional model weights exclusively
on 3 of the competition Open maps.
\begin{inparaenum}[(1)]
    \item \texttt{NoWhereToRun9x8} was one of the 6 maps the base model trained on;
    however, the base agent couldn't beat CoacAI or Mayari on this map. The map was very
    different from the other 5 maps consisting of a wall of resources in the middle
    separated opponents.
    \item \texttt{DoubleGame24x24} was larger than the maps the base model trained on
    and
    \item \texttt{BWDistantResources32x32} was larger still.
\end{inparaenum}
All 3 transfer learning runs used the same schedule:
\begin{inparaenum}[(1)]
    \item warmup of sparse, win-loss reward weights linearly transitioned to a mixture
    of both shaped and sparse rewards,
    \item middle phase of mixed rewards,
    \item end phase of sparse rewards again at a lower learning rate.
\end{inparaenum}
\texttt{NoWhereToRun9x8} had an additional fine-tuning using only the win-loss reward.
These transfer learned agents exceeded 90\% win-rate on their respective maps,
significant improvements over the base model (especially on
\texttt{BWDistantResources32x32} which started below 10\%)

\subsection{squnet training}
We trained the squnet moels with fewer steps because of time-constraints and skipped the
predominantely cost-based reward phase because it didn't seem to help with training the
base model. The 2 models trained from randomly initialized weights for maps of up to 32 (map32)
and 64 (map64) managed only a 40\% win-rate, never beating CoacAI or Mayari. These
models were polies of last resort.

We also fine-tuned the squnet-map32 model on \texttt{BWDistantResources32x32}
exclusively using the same sparse fine-tuning schedule. This fine-tuned model achieved
85\% win-rate, beating Mayari half the time, but never beating CoacAI.

\subsection{Behavior cloning bootstrapped training}
We submitted \agentName\ with the above initially randomized training followed up with
fine-tuning and transfer learning. However, in follow-up work, we wanted to see if it
was possible to train an agent that
\begin{inparaenum}[(1)]
    \item didn't require the shaped rewards and reward scheduling,
    \item could be trained in fewer steps and less time, and
    \item could defeat prior competition winners on the largest maps.
\end{inparaenum}
We opted for a neural architecture between DoubleCone and squnet: a single-nested downscaling residual
block each of stride 4, so the bottom block scales the input down to 1/16th the original
size. At each downscaling level, there was multiple residual blocks (6 at full
resolution, split evenly by the downscaling block; 4 at 1/4 resolution, split evenly by
the 1/16 downscaling block; and 4 at 1/16 resolution). This architecture theoretically
has a 128x128 receptive field while using 25\% fewer operations at inference time. On
the largest competition Open map (\texttt{(4)BloodBath.scmB}), this is 6x more
computations than squnet-64. Therefore, this neural architecture won't be usable in a
competition given the same hardware constraints as the 2023 competition.

Initially, we tried a similar training strategy to \agentName\ where the model is
trained on 16x16 maps and then reusing that model as a starting point to transer to
larger maps. However, we only managed a 60\% win-rate on
\texttt{BWDistantResources32x32} and less than 20\% win-rate on
\texttt{(4)BloodBath.scmB} before terminating training.

Instead, similar to \citet{Vinyals2019GrandmasterLI}, we used behavior cloning to
bootstrap the model. Instead of 3 rewards and value heads, we used only the win-loss
reward. However, microRTS doesn't have human replays, so we used playthroughs of the
2021 competition winner (Mayari) playing against Mayari, CoacAI (2020 competition
winner), and lightRush (baseline scripted bot and 2017 competition winner). Instead of
generating an offline replay dataset, we set the microRTS environment to play bots
against each other and these observations and actions were fed into rollouts used for
behavior cloning the policy and fitting the value heads:
\begin{align}
    L^{BC+VF}(\theta) &= \left[ L^{BC}(\theta) - c_1 L^{VF} \right], \\
    L^{BC}(\theta) &= \frac{1}{\|a_t\|} \log \pi_{\theta}(a_t|s_t),
\end{align}
where $c_1$ and $L^{VF}$ are the same as in the PPO loss function. The policy behavior
cloning loss is the cross-entropy loss between the policy logits and the actions taken
by the Mayari bot. We found scaling the loss by the number of units that could take an
action critical for behavior cloning to train.

We trained 3 behavior cloned models for the 3 Open competition maps sizes (16x16,
32x32, and 64x64) using the same maps for each map size as \agentName\ training. The
64x64 model used the weights of the 32x32 model as a starting point, while the other two
models were randomly initialized. We then used PPO to fine-tune the behavior cloned
models on the same maps.

\subsection{Tournament setup}
\subsubsection{Single player round-robin tournament}
\agentName\ plays against 4 opponents on the 8 Open competition maps in 100 matches (50
each as player 1 and 2). The 4 opponents are
\begin{inparaenum}[(1)]
    \item baseline POWorkerRush,
    \item baseline and 2017 competition winner POLightRush,
    \item 2020 competition winner CoacAI, and
    \item last competition (2021) winner Mayari.
\end{inparaenum}
POWorkerRush, POLightRush, and CoacAI use A* pathfinding, which is default for
competitions.  Win rates are percentages of wins where draws count as 0.5 wins for each
player. The single player round-robin tournament was run on a 2018 Mac Mini with Intel i7-8700B
CPU (6-core, 3.2GHz) with PyTorch limited to 6 threads. Timeouts were set to 100 ms. If
an agent took 20ms over the deadline (12 0ms total), the game was terminated and the win
awarded to the opponent.

\subsubsection{IEEE-CoG2023 microRTS competition}
The IEEE-CoG2023 microRTS competition is a round-robin tournament on 12 maps: 8 Open
(known before the competition) and 4 Hidden (hidden from competitors until after the
competition). The winner is the agent with the highets win rate on the 8 Open maps. For
this competition, a total of 11 agents were submitted: 9 programatic policies, 1
synthesized programatic policy, and \agentName\. The competition also had 5
baselines:
\begin{inparaenum}[(1)]
    \item RandomBiasedAI (performs actions randomly, biased towards attacking if able),
    \item NaiveMCTS (a simple Monte Carlo tree search agent that searches until reaching
    the timelimit)
    \item POWorkerRush,
    \item POLightRush,
    \item 2L (programmatic strategies generated by \citet{Moraes2023ChoosingWY}, the
    competition organizers).
\end{inparaenum}
The prior competition winner Mayari was also included in the round-robin. Neither Mayari
nor the baselines could win the competition.

Each agent played every other agent on each map 20 times (10 each as player 1 and 2).
Timeouts were disabled for the competition, but the Java-side of \agentName\ would skip
its turn (submitting no actions) if 100 ms has elapsed. On
\texttt{BWDistantResources32x32}, \agentName\ chose between the DoubleCone and squnet
finetuned models by running both models on the first observation 100 times each and
choosing the better model that completes each step on average within 75 ms.

\section{Results}
\subsection{Single player round-robin tournament}
\begin{table}[ht]
    \centering
    \begin{threeparttable}
    \caption{Single player round-robin tournament win rates. Win rates above 50\% are bolded. (D)
    DoubleCone model used. (S) squnet model used. On
    \texttt{BWDistantResources32x32}, squnet (S) model only played 20 games per map per opponent
    (10 each as player 1 and 2).}
    \label{tab:single-player-winrate}
    \begin{tabular}{lcccc|c}
    \toprule
     & \textbf{WorkerRush} & \textbf{LightRush} & \textbf{CoacAI} & \textbf{Mayari} & \textbf{Overall} \\
    \midrule
    basesWorkers8x8A & \textbf{95} & \textbf{100} & \textbf{99} & \textbf{100} & \textbf{99} \\
    FourBasesWorkers8x8 & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{98} & \textbf{100} \\
    NoWhereToRun9x8 & \textbf{100} & \textbf{100} & \textbf{93} & \textbf{99} & \textbf{98} \\
    basesWorkers16x16A & \textbf{100} & \textbf{100} & \textbf{90} & \textbf{98} & \textbf{97} \\
    TwoBasesBarracks16x16 & \textbf{100} & \textbf{89} & \textbf{99} & \textbf{100} & \textbf{97} \\
    DoubleGame24x24 & \textbf{100} & \textbf{98} & \textbf{94} & \textbf{100} & \textbf{98} \\
    BWDistantResources32x32 & \begin{tabular}[c]{@{}c@{}}\textbf{99} (D) \\ \textbf{93}
    (S)\end{tabular}  & \begin{tabular}[c]{@{}c@{}}\textbf{90} (D) \\ \textbf{73}
    (S)\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{88} (D) \\ 23
    (S)\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}\textbf{99} (D) \\ \textbf{58} (S)\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}\textbf{94}\tnote{\P} (D) \\ \textbf{61} (S)\end{tabular} \\
    (4)BloodBath.scmB & \textbf{98} & 0 & 0 & 0 & 25\tnote{\dag} \\
    \hline
    \textbf{AI Average}\tnote{*} & \textbf{99} & \textbf{85} & \textbf{83} & \textbf{87} & \textbf{88} \\
    \end{tabular}
    \begin{tablenotes}
    \item[*] AI Average uses the DoubleCone (D) results from BWDistantResources32x32.
    \item[\P] \agentName\ lost 0.25\% of matches (1 match) by timeout.
    \item[\dag] \agentName\ lost  1\% of matches (4 matches) by timeout.
    \end{tablenotes}
    \end{threeparttable}
\end{table}

In a single player round-robin tournament on the Open competition maps (Table~\ref{tab:single-player-winrate}), \agentName\
beat the competition winners of 2021 (Mayari), 2020 (CoacAI), and 2017 (POLightRush,
baseline) on 7 of the 8 maps (winning over 97\% of games on these maps). \agentName\
could only beat the POWorkerRush baseline bot on the largest map,
\texttt{(4)BloodBath.scmB}. The DistantResources-finetuned squnet model winrate is denoted with (S) in
Table~\ref{tab:single-player-winrate}. It performed worse than the DoubleCone (D) model
across all opponents, but maintained an over 50\% win rate against all but CoacAI. Timeouts were enabled for the tournament, but they didn't affect results significantly:
only 1\% of \texttt{(4)BloodBath.scmB} and 0.25\% of \texttt{BWDistantResources32x32}
timedout, all against \agentName.

\subsection{IEEE-CoG2023 microRTS competition results}
\agentName\ was declared the winner with the highest win rate (72\%) across all
submissions (Table~\ref{tab:competition-winrate}).
\agentName\ had a higher win rate than all but one baseline: \texttt{2L} (76\%). \agentName\
had a 10\% lower overall win rate than Mayari despite having a higher heads up win rate
(\agentName\ defeated Mayari 62\% as player 1 and 68\% as player 2). On a 1 vs 1 basis,
\agentName\ had a win rate over 50\% win rate versus every opponent.

\begin{table}[ht]
    \caption{Win rates of selection of agents in the IEEE-CoG2023 microRTS competition.
    Player 1 is the row agent and player 2 is the column agent. Each win rate value is
    the percentage of games won by player 1 (the row agent). Cells are bolded if the win
    rate is higher than the opponent's row win rate. For example, \agentName vs
    ObiBotKenobi is bolded because 49\% higher than 47\%, thus meaning the combined player 1
    and 2 win rate is higher for \agentName\ ObiBotKenobi (51\%). Overall includes all agents, including those not shown.}
    \label{tab:competition-winrate}
    \begin{center}
    \begin{tabular}{lcccccc|c}
    \toprule
    & \begin{sideways} mayari \end{sideways} 
    & \begin{sideways} 2L \end{sideways} 
    & \begin{sideways} \textbf{\agentName} \end{sideways} 
    & \begin{sideways} ObiBotKenobi \end{sideways} 
    & \begin{sideways} POLightRush \end{sideways} 
    & \begin{sideways} POWorkerRush \end{sideways} 
    & \begin{sideways} Overall \end{sideways} \\
    \midrule
    mayari (2021 winner) & - & \textbf{53} & 32 & \textbf{73} & \textbf{88} & \textbf{75} & 82 \\
    2L (baseline) & 51 & - & 39 & \textbf{50} & \textbf{75} & \textbf{88} & 76 \\
    \textbf{\agentName} (2023 winner) & \textbf{62} & \textbf{59} & - & \textbf{49} & \textbf{64} & \textbf{78} & 72 \\
    ObiBotKenobi (2023 2nd place) & 39 & 29 & 47 & - & \textbf{58} & \textbf{65} & 66 \\
    POLightRush (baseline) & 0 & 25 & 29 & 38 & - & \textbf{69} & 55 \\
    POWorkerRush (baseline) & 13 & 13 & 21 & 29 & 38 & - & 53 \\
    \bottomrule
    \end{tabular}
\end{center}
\end{table}

\begin{table}[ht]
    \caption{\agentName\ win rates in 2023 competition by opponent and map. Bolded cells are win rates over 50\%. Overall includes all agents, including those not shown.}
    \label{tab:competition-winrate-by-map}
    \begin{center}
    \begin{tabular}{lccccc|c}
    \toprule
    & \begin{sideways}POWorkerRush\end{sideways} & \begin{sideways}POLightRush\end{sideways} & \begin{sideways}ObiBotKenobi\end{sideways} & \begin{sideways}2L\end{sideways} & \begin{sideways}mayari\end{sideways} & \begin{sideways}Overall\end{sideways}\\ 
    \midrule
    basesWorkers8x8A & \textbf{60} & \textbf{70} & \textbf{60} & \textbf{60} & \textbf{60} & \textbf{66}\\ 
    FourBasesWorkers8x8 & \textbf{100} & \textbf{100} & 21 & \textbf{97} & \textbf{100} & \textbf{95}\\ 
    NoWhereToRun9x8 & \textbf{90} & \textbf{85} & \textbf{83} & \textbf{70} & \textbf{70} & \textbf{84}\\
    basesWorkers16x16A & \textbf{100} & \textbf{100} & \textbf{95} & \textbf{100} & \textbf{100} & \textbf{100}\\
    TwoBasesBarracks16x16 & \textbf{80} & \textbf{80} & 10 & \textbf{70} & \textbf{80} & \textbf{75}\\
    DoubleGame24x24 & \textbf{80} & \textbf{75} & \textbf{78} & \textbf{80} & \textbf{75} & \textbf{80}\\ 
    BWDistantResources32x32 & 50 & 30 & 35 & 3 & 35 & \textbf{54}\\
    (4)BloodBath.scmB & \textbf{70} & 0 & 28 & 0 & 0 & 34\\ 
    \hline
    AI Average & \textbf{79} & \textbf{68} & \textbf{51} & \textbf{60} & \textbf{65} & \textbf{74}\\ 
    \end{tabular}
    \end{center}
\end{table}

As expected from the single player round-robin tournament, \agentName\ had higher win
rates on smaller rates and a dismal win rate on the largest maps
(Table~\ref{tab:competition-winrate-by-map}). However, the competition agent
underperformed against agents already benchmarked in the single player round-robin
(about 14-19\% lower win rate against each agent), even accounting for the likely use of
the weaker squnet model.  Breaking down by map, \agentName\ underperformed against
benchmarked agents by 20-40\% on most of the maps.

The competition ran the tournament in jobs splitting each map into 5 or 10 jobs where
each job would run a complete round-robin with all agents on that map playing 2 or 1
games as player 1 and 2 each. On \texttt{basesWorkers8x8A}, which \agentName\
underperformed by almost 40\%, the competition had 5 jobs. On the first 3 jobs,
\agentName won nearly every game. On the last 2 jobs, \agentName\ lost nearly every
game. This suggests \agentName's performance could depend on the job environment.

\subsection{Behavior cloning}
We created 2 additional agents from the behavior cloning (\bcAgent) and the following
PPO finetuning (\bcPPOAgent). Each agent consisted of the models trained on their
respective map sizes (16x16, 32x32, and 64x64). These agents do not have any
map-specific models. These agents went through the same single player round-robin
tournmanent as \agentName; however, playing only each opponent on each map in 20 games
(10 each as player 1 and 2).

\begin{table}[ht]
    \caption{\bcAgent\ win rate in single player round-robin tournament (20 games per map per opponent).}
    \label{tab:bc-winrate}
    \begin{center}
    \begin{tabular}{lcccc|c}
    \toprule
     & POWorkerRush & POLightRush & CoacAI & Mayari & Overall \\
    \midrule
    basesWorkers8x8A & \textbf{60} & \textbf{100} & \textbf{90} & \textbf{50} & \textbf{75} \\
    FourBasesWorkers8x8 & \textbf{100} & \textbf{100} & \textbf{85} & \textbf{65} & \textbf{88} \\
    NoWhereToRun9x8 & \textbf{100} & \textbf{100} & \textbf{83} & \textbf{55} & \textbf{85} \\
    basesWorkers16x16A & 10 & \textbf{100} & \textbf{100} & 28 & \textbf{60} \\
    TwoBasesBarracks16x16 & \textbf{100} & \textbf{100} & 43 & 20 & \textbf{66} \\
    DoubleGame24x24 & 0 & \textbf{100} & \textbf{100} & 30 & \textbf{58} \\
    BWDistantResources32x32 & 48 & \textbf{100} & \textbf{100} & \textbf{65} & \textbf{78} \\
    (4)BloodBath.scmB & \textbf{100} & \textbf{63} & 20 & 40 & \textbf{56} \\
    \hline
    AI Total & \textbf{65} & \textbf{96} & \textbf{78} & 44 & 71 \\
    \end{tabular}
    \end{center}
\end{table}

\begin{table}[ht]
    \centering
    \caption{\bcPPOAgent\ win rate in single player round-robin tournament (20 games per map per opponent).}
    \label{tab:bcppo-winrate}
    \begin{center}
    \begin{tabular}{lcccc|c}
    \toprule
     & POWorkerRush & POLightRush & CoacAI & Mayari & Overall \\
    \midrule
    basesWorkers8x8A & \textbf{100} & \textbf{100} & \textbf{85} & \textbf{100} & \textbf{96} \\
    FourBasesWorkers8x8 & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} \\
    NoWhereToRun9x8 & \textbf{100} & \textbf{100} & \textbf{88} & \textbf{85} & \textbf{93} \\
    basesWorkers16x16A & \textbf{100} & \textbf{100} & \textbf{95} & \textbf{100} & \textbf{99} \\
    TwoBasesBarracks16x16 & \textbf{100} & 0 & \textbf{100} & \textbf{100} & \textbf{75} \\
    DoubleGame24x24 & \textbf{95} & \textbf{90} & \textbf{100} & \textbf{100} & \textbf{96} \\
    BWDistantResources32x32 & \textbf{100} & \textbf{95} & \textbf{90} & \textbf{100} & \textbf{96} \\
    (4)BloodBath.scmB & \textbf{100} & \textbf{95} & 0 & 0 & 49 \\
    \hline
    AI Total & \textbf{100} & \textbf{85} & \textbf{82} & \textbf{86} & \textbf{88} \\
    \end{tabular}
\end{center}
\end{table}

Table~\ref{tab:bc-winrate} shows \bcAgent\ had a 71\% win rate doing extremly well
against the POLightRush baseline (96\%, better than \agentName) and 44\% win rate
against Mayari. \bcAgent\ does not dominate any map, but does manage non-zero win rates
on the largest map (\agentName\ could only beat the POWorkerRush baseline).

Once finetuned with PPO, \bcPPOAgent\ obtains an \agentName\ comparable 88\% win rate
(Table~\ref{tab:bcppo-winrate}). \bcPPOAgent\ generally improves upon \bcAgent's win
rates on each map and against each opponent. However, the biggest exceptions are
POLightRush on \texttt{TwoBasesBarracks16x16} (from 100\% to 0\%) and the largest map
\texttt{(4)BloodBath.scmB} where the finetuned model can no longer beat CoacAI and
Mayari.

\section{Discussion}
\subsection{Improving inference time in microRTS competitions}
\agentName\ underperformed at the 2023 competition on several maps in a way that
suggests the job environment could sometimes run slower. We worked with the competition
organizers to reduce the chance of timeouts by tweaking the number of threads available
to PyTorch during inference. However, it was difficult to reproduce the same results on
the competition servers in our development environments.

Improving inference time is critical for creating agents with more reliable performance
in the microRTS competitions, leading to fewer underperformances between benchmarks and
competition results. We suggest 3 improvements (2 for agents, and 1 for the competition
organizers):
\begin{inparaenum}[(1)]
    \item use fast inference runtime providers (like OpenVINO for ONNX Runtime),
    \item continue to train agents using the smaller squnet models (possibly with
    behavior cloning to bootstrap training), and
    \item add an overtime budget to the competition to better handle environment
    instabilities.
\end{inparaenum}
For DoubleCone, \citet{Ferdinand2021doublecone} found using OpenVINO could have made
inference 2-3 times faster in the LUX competition. 

\bibliography{iclr2024}
\bibliographystyle{iclr2024_conference}

\clearpage

\appendix
\section{Competition details}
\begin{table}[H]
    \centering
    \begin{threeparttable}
        \caption{Open competition maps. Representation column is the size of the vectorized observation in \agentName.}
        \label{tab:open-maps}
            \begin{tabular}{>{\centering\arraybackslash}m{4cm}>{\centering\arraybackslash}m{2cm}>{\centering\arraybackslash}m{2cm}>{\centering\arraybackslash}m{2.25cm}}
                \multicolumn{1}{c}{\bf Name}  &\multicolumn{1}{c}{\bf Size}
                &\multicolumn{1}{c}{\bf Representation}
                &\multicolumn{1}{c}{\bf Start} \\ \hline
                basesWorkers8x8A & 8x8 & 16x16 &
                \includegraphics[width=2.25cm]{supplemental/vsMayari/basesWorkers8x8A.png} \\
                FourBasesWorkers8x8 & 8x8 & 16x16 &
                \includegraphics[width=2.25cm]{supplemental/vsMayari/FourBasesWorkers8x8.png} \\
                NoWhereToRun9x8 & 9x8 & 12x12\tnote{*} or 16x16\tnote{\P} &
                \includegraphics[width=2.25cm]{supplemental/vsMayari/NoWhereToRun9x8.png} \\
                basesWorkers16x16A & 16x16 & 16x16 &
                \includegraphics[width=2.25cm]{supplemental/vsMayari/basesWorkers16x16A.png} \\
                TwoBasesBarracks16x16 & 16x16 & 16x16 &
                \includegraphics[width=2.25cm]{supplemental/vsMayari/TwoBasesBarracks16x16.png}
                \\
                DoubleGame24x24 & 24x24 & 24x24\tnote{\dag} or 32x32\tnote{\ddag} &
                \includegraphics[width=2.25cm]{supplemental/vsMayari/DoubleGame24x24.png} \\
                BWDistantResources32x32 & 32x32 & 32x32 &
                \includegraphics[width=2.25cm]{supplemental/vsMayari/BWDistantResources32x32.png}
                \\
                (4)BloodBath.scmB & 64x64 & 64x64 &
                \includegraphics[width=2.25cm]{supplemental/vsMayari/(4)BloodBath.png} \\
            \end{tabular}
            \begin{tablenotes}
                \item[*] ppo-Microrts-finetuned-NoWhereToRun-S1-best uses a 12x12
                representation
                \item[\P] ppo-Microrts-A6000-finetuned-coac-mayari-S1-best
                \item[\dag] ppo-Microrts-finetuned-DoubleGame-shaped-S1-best uses a 24x24
                    representation.
                \item[\ddag] ppo-Microrts-squnet-map32-128ch-selfplay-S1-best pads the
                    observation to 32x32.
            \end{tablenotes}
    \end{threeparttable}
\end{table}

\begin{table}[h]
    \caption{Policy networks used by \agentName}
    \label{tab:policy-networks}
    \begin{center}
        \begin{tabular}{|p{0.5\linewidth}|p{0.5\linewidth}|}
            \multicolumn{1}{c}{\bf Network}  &\multicolumn{1}{c}{\bf Usage} \\
            \hline
            ppo-Microrts-finetuned-NoWhereToRun-S1-best & NoWhereToRun9x8 \\ \hline
            ppo-Microrts-A6000-finetuned-coac-mayari-S1-best & All other maps of size 16x16 and smaller \\ \hline
            ppo-Microrts-finetuned-DoubleGame-shaped-S1-best & DoubleGame24x24 \\ \hline
            ppo-Microrts-finetuned-DistantResources-shaped-S1-best & BWDistantResources32x32 if completion time under 75 ms \\ \hline
            ppo-Microrts-squnet-DistantResources-128ch-finetuned-S1-best & BWDistantResources32x32 if completion time above 75 ms \\ \hline
            ppo-Microrts-squnet-map32-128ch-selfplay-S1-best & All other maps where longest dimension is between 17-32 \\ \hline
            ppo-Microrts-squnet-map64-64ch-selfplay-S1-best & Maps where the longest
            dimension is over 32 \\
            \hline
        \end{tabular}
    \end{center}
\end{table}

\section{Neural network architecture}
\begin{figure}[H]
    \begin{center}
        \includegraphics[width=0.6\linewidth]{figures/DoubleCone.png}
    \end{center}
    \caption{DoubleCone(4, 6, 4) neural network architecture.}
    \label{fig:doublecone}
\end{figure}

\begin{figure}[h]
    \begin{center}
        \includegraphics[width=0.3\linewidth]{figures/SqueezeExcitation.png}
    \end{center}
    \caption{ResBlock used in DoubleCone, squnet32, and squnet64. The residual block is similar to a standard residual block but inserts a Squeeze-Excitation block after the convolutional layers and before the residual connection.}
    \label{fig:squeezeexcitation}
\end{figure}

\begin{figure}[h]
    \begin{center}
        \includegraphics[width=0.7\linewidth]{figures/ValueHeads.png}
    \end{center}
    \caption{Value heads used in (from left to right) DoubleCone, squnet32, and
    squnet64. The AdaptiveAvgPool2d layer allows the network to be used on other map
    sizes.}
    \label{fig:valueheads}
\end{figure}

\begin{figure}[H]
    \begin{center}
        \includegraphics[width=0.55\linewidth]{figures/squnet64.png}
    \end{center}
    \caption{squnet64 neural network architecture. Instead of one downscaling block as in DoubleCone, this network downscales 3 times. This aggressive downscaling reduces the number of computations for larger maps, while theoretically supporting a large receptive field.}
    \label{fig:squnet}
\end{figure}
        
\begin{table}[h]
    \centering
    \begin{threeparttable}
        \caption{Comparison of different architectures}
        \label{table:architectureBreakdown}
        \begin{tabular}{cccc}
            & \textbf{DoubleCone} & \textbf{squnet-map32\tnote{\P}} & \textbf{squnet-map64} \\ [0.5ex] 
            \hline
            Levels & 2 & 4 & 4 \\
            \hline
            Encoder residual blocks/level & [4, 6] & [1, 1, 1, 1] & [1, 1, 1, 1] \\
            \hline
            Decoder residual blocks/level & [4] & [1, 1, 1] & [1, 1, 1] \\
            \hline
            Stride/level & [4] & [2, 2, 4] & [2, 4, 4] \\
            \hline
            Deconvolution strides/level & [[2, 2]\tnote{*}] & [2, 2, 4] & [2, 4, 4] \\
            \hline
            Channels/level & [128, 128] & [128, 128, 128, 128] & [64, 64, 64, 64] \\
            \hline
            Trainable parameters & 5,014,865 & 3,584,657 & 1,420,625 \\
            \hline
            MACs\tnote{\dag} & \begin{tabular}[c]{@{}c@{}}0.70B (16x16)\tnote{\ddag} \\ 0.40B (12x12)\tnote{\S} \\ 1.58B (24x24) \\ 2.81B (32x32)\end{tabular} & 1.16B (32x32) & 1.41B (64x64) \\ 
            \hline
        \end{tabular}
        \begin{tablenotes}
            \item[\P] Used by ppo-Microrts-squnet-DistantResources-128ch-finetuned-S1-best and ppo-Microrts-squnet-map32-128ch-selfplay-S1-best. 
            \item[*] 2 stride-2 transpose convolutions to match the 1 stride-4 convolution.
            \item[\dag] Multiply-Accumulates for computing actions for a single observation.
            \item[\ddag] All maps smaller than 16x16 (except NoWhereToRun9x8) are padded with walls up to 16x16. 
            \item[\S] NoWhereToRun9x8 is padded with walls up to 12x12.
        \end{tablenotes}
    \end{threeparttable}
\end{table}

\section{Initial training details}

\begin{table}[H]
    \centering
    \begin{threeparttable}
    \caption{Comparison of Initial Training, Shaped Fine-Tuning, and Sparse Fine-Tuning Parameters}
    \label{tab:training-parameters}
    \begin{tabular}{lccc}
    \toprule
    Parameter & Initial Training & Shaped Fine-Tuning & Sparse Fine-Tuning \\
    \midrule
    Steps & 300M &   100M &   100M \\
    n\_envs & 24 &    \textquotedbl &    \textquotedbl \\
    Rollout Steps Per Env & 512 &   \textquotedbl &   \textquotedbl \\
    Minibatch Size & 4096 &   \textquotedbl &   \textquotedbl \\
    Epochs Per Rollout & 2 &   \textquotedbl &   \textquotedbl \\
    Gamma & [0.99, 0.999, 0.999]\tnote{*} &   \textquotedbl &   \textquotedbl \\
    gae\_lambda & [0.95, 0.99, 0.99]\tnote{\P} &   \textquotedbl &   \textquotedbl \\
    Clip Range & 0.1 &   \textquotedbl &   \textquotedbl \\
    Clip Range VF & 0.1 &    \textquotedbl &   \textquotedbl \\
    VF Coef Halving\tnote{‡} & True  &    \textquotedbl &   \textquotedbl \\
    Max Grad Norm &  0.5 &   \textquotedbl &   \textquotedbl \\   
    Latest Selfplay Envs   &   12 &                      \textquotedbl &                      \textquotedbl \\
    Old Selfplay Envs   &   12 &                      0 &                      0 \\
    Bots   &   none & CoacAI: 12 & \begin{tabular}[c]{@{}c}CoacAI: 6\\ Mayari: 6\end{tabular} \\
    Maps   &   \begin{tabular}[c]{@{}c}basesWorkers16x16A \\ TwoBasesBarracks16x16 \\
    basesWorkers8x8A \\ FourBasesWorkers8x8 \\ NoWhereToRun9x8 \\
    EightBasesWorkers16x16\tnote{\dag} \end{tabular} &  \textquotedbl & \textquotedbl \\
    \bottomrule
    \end{tabular}
    \begin{tablenotes}
    \item[\textquotedbl] Same value as cell to left.
    \item[*] Value per value head (shaped, win-loss, cost-based).
    \item[\P] Multiply v\_{loss} by 0.5, as done in CleanRL.
    \item[\dag] Map not used in competition.
    \end{tablenotes}
    \end{threeparttable}
\end{table}

\begin{table}[H]
    \centering
    \begin{threeparttable}
    \caption{Initial Training schedule from a randomly initialized model}
    \label{tab:initial-training-schedule}
    \begin{tabular}{lccccr}
    \toprule
     & Phase 1 & Transition 1→2\tnote{*} & Phase 2 & Transition 2→3\tnote{*} & Phase 3 \\
    \midrule
    steps & 90M & 60M & 30M & 60M & 60M \\
    reward weights\tnote{†} & [0.8, 0.01, 0.19] &  & [0, 0.5, 0.5] &  & [0, 0.99, 0.01] \\
    $c_1$ (value loss coef)\tnote{†} & [0.5, 0.1, 0.2] &  & [0, 0.4, 0.4] &  & [0, 0.5, 0.1]\\
    $c_2$ (entropy coef)& 0.01 & & 0.01 & & 0.001 \\
    learning rate & $10^{-4}$ & & $10^{-4}$ & & $5 \times 10^{-5}$\\
    \bottomrule
    \end{tabular}
    \begin{tablenotes}
       \item[*] Values are linearly interpolated between phases based on step count.
       \item[†] Listed weights are for the shaped, win-loss, cost-based values, respectively.
    \end{tablenotes}
    \end{threeparttable}
\end{table}

\begin{table}[H]
    \centering
    \begin{threeparttable}
    \caption{Shaped Fine-tuning schedule starting with weights from Initial Training output}
    \label{tab:shaped-finetuning-schedule}
    \begin{tabular}{lccccc}
    \toprule
     & Start & Transition →1\tnote{*} & Phase 1 & Transition 1→2\tnote{*} & Phase 2 \\
     \midrule
    steps & & 5M & 30M & 20M & 45M \\
    reward weights\tnote{†} & [0, 0.99, 0.01] & & [0, 0.5, 0.5] & & [0, 0.99, 0.01] \\
    $c_1$ (value loss coef)\tnote{†} & [0, 0.4, 0.2] & & [0, 0.4, 0.4] & & [0, 0.5, 0.1]\\
    $c_2$ (entropy coef) & 0.01 & & 0.01 & & 0.001 \\
    learning rate & $10^{-5}$ & & $5 \times 10^{-5}$ & & $5 \times 10^{-5}$\\
    \bottomrule
    \end{tabular}
    \begin{tablenotes}
       \item[*] Values are linearly interpolated between phases based on step count.
       \item[†] Listed weights are for the shaped, win-loss, cost-based values, respectively.
    \end{tablenotes}
    \end{threeparttable}
\end{table}

\begin{table}[H]
    \centering
    \begin{threeparttable}
    \caption{Sparse Fine-tuning schedule starting with weights from Shaped Fine-tuning output}
    \label{tab:sparse-finetuning-schedule}
    \begin{tabular}{lcccc}
    \toprule
     & Phase 1 & Transition 1→2\tnote{*} & Phase 2 \\
     \midrule
    steps & 30M & 40M & 30M \\
    reward weights\tnote{†} & [0, 0.99, 0.01] &  & [0, 0.99, 0.01] \\
    $c_1$ (value loss coef)\tnote{†} & [0, 0.5, 0.1] &  & [0, 0.5, 0.1]\\
    $c_2$ (entropy coef)) & 0.001 & & 0.0001 \\
    learning rate & $5 \times 10^{-5}$ & & $10^{-5}$ \\
    \bottomrule
    \end{tabular}
    \begin{tablenotes}
       \item[*] Values are linearly interpolated between phases based on step count.
       \item[†] Listed weights are for the shaped, win-loss, cost-based values, respectively.
    \end{tablenotes}
    \end{threeparttable}
\end{table}

\section{Transfer learning details}

\begin{table}[H]
    \centering
    \begin{threeparttable}
    \caption{Transfer learning schedule starting from ppo-Microrts-A6000-finetuned-coac-mayari-S1-best model}
    \label{tab:transfer-learning-schedule}
    \begin{tabular}{lccccc}
    \toprule
     & Start & Transition →1\tnote{*} & Phase 1 & Transition 1→2\tnote{*} & Phase 2 \\
     \midrule
    steps & & 5M & 30M & 20M & 45M \\
    reward weights\tnote{†} & [0, 0.99, 0.01] & & [0.4, 0.5, 0.1] & & [0, 0.99, 0.01] \\
    $c_1$ (value loss coef)\tnote{†} & [0.2, 0.4, 0.2] & & [0.3, 0.4, 0.1] & & [0, 0.5, 0.1]\\
    $c_2$ (entropy coef) & 0.01 & & 0.01 & & 0.0001 \\
    learning rate & $5 \times 10^{-5}$ & & $7 \times 10^{-5}$ & & $10^{-5}$ \\
    \bottomrule
    \end{tabular}
    \begin{tablenotes}
       \item[*] Values are linearly interpolated between phases based on step count.
       \item[†] Listed weights are for the shaped, win-loss, cost-based values, respectively.
    \end{tablenotes}
    \end{threeparttable}
\end{table}

\section{squnet learning details}
\begin{table}[H]
    \centering
    \begin{threeparttable}
    \caption{squnet training parameters}
    \label{tab:squnet-training-parameters}
    \begin{tabular}{lccc}
    \toprule
    Parameter & map32 & map32-DistantResources & map64 \\
    \midrule
    Steps & 200M & 100M & 200M \\
    n\_envs & 24 & \textquotedbl & \textquotedbl \\
    Rollout Steps Per Env & 512 & 512 & 256 \\
    Minibatch Size & 2048 & 2048 & 258 \\
    Clip Range & 0.1 & \textquotedbl & \textquotedbl \\
    Clip Range VF & none & \textquotedbl & \textquotedbl \\
    Latest Selfplay Envs & 12 & \textquotedbl & \textquotedbl \\
    Old Selfplay Envs & 6 & 6 & 4 \\
    Bots & \begin{tabular}[c]{@{}c}coacAI: 3 \\ mayari: 3 \end{tabular} & \begin{tabular}[c]{@{}c}coacAI: 3 \\ mayari: 3 \end{tabular} & \begin{tabular}[c]{@{}c}coacAI: 4 \\ mayari: 4 \end{tabular} \\
    Maps & \begin{tabular}[c]{@{}c}DoubleGame24x24 \\ BWDistantResources32x32 \\ chambers32x32\tnote{*} \end{tabular} & BWDistantResources32x32 & \begin{tabular}[c]{@{}c}BloodBath.scmB \\ BloodBath.scmE\tnote{*}\end{tabular} \\   
    \bottomrule
    \end{tabular}
    \begin{tablenotes}
    \item[\textquotedbl] Same value as cell to left.
    \item[*] Not compeition Open maps.
    \end{tablenotes}
    \end{threeparttable}
\end{table}

\begin{table}[H]
    \centering
    \begin{threeparttable}
    \caption{squnet training schedule starting with randomly initialized weights}
    \label{tab:squnet-training-schedule}
    \begin{tabular}{lcccc}
    \toprule
     & Phase 1 & Transition 1→2\tnote{*} & Phase 2 \\
     \midrule
    steps & 100M & 60M & 40M \\
    reward weights\tnote{†} & [0.8, 0.01, 0.19] &  & [0, 0.99, 0.01] \\
    $c_1$ (value loss coef)\tnote{†} & [0.5, 0.1, 0.2] &  & [0, 0.5, 0.1]\\
    $c_2$ (entropy coef) & 0.01 & & 0.001 \\
    learning rate & $10^{-4}$ & & $5 \times 10^{-5}$ \\
    \bottomrule
    \end{tabular}
    \begin{tablenotes}
       \item[*] Values are linearly interpolated between phases based on step count.
       \item[†] Listed weights are for the shaped, win-loss, cost-based values, respectively.
    \end{tablenotes}
    \end{threeparttable}
\end{table}

\section{Behavior cloning bootstrapped training details}
\begin{table}[H]
    \centering
    \begin{threeparttable}
    \caption{Neural architecture for behavior cloning and PPO fine-tuned training}
    \label{tab:bc-architecture}
    \begin{tabular}{lc}
    \toprule
                                 & deep16-128 \\
    \midrule
    Levels                      & 3  \\
    Encoder residual blocks/level & [3, 2, 4] \\
    Decoder residual blocks/level & [3, 2] \\
    Stride per level            & [4, 4] \\
    Deconvolution strides per level & [[2, 2], [2, 2]]$^{*}$\\
    Channels per level          & [128, 128, 128] \\
    Trainable parameters        & 5,027,279 \\
    MACs$^{†}$ (16x16)          & 0.52B \\
    MACs$^{†}$ (64x64)          & 8.40B \\
    \bottomrule
    \end{tabular}
    \begin{tablenotes}
       \item[*] 2 stride-2 transpose convolutions to match the 1 stride-4 convolution.
       \item[†] Multiply-Accumulates for computing actions for a single observation.
    \end{tablenotes}
    \end{threeparttable}
\end{table}

\section{Additional IEEE-CoG2023 microRTS competition details}
\begin{table}[H]
    \caption{Win rates of all agents in the IEEE-CoG 2023 microRTS competition.  Player 1 is the row agent and player 2 is the column agent. Each win rate value is the percentage of games won by player 1 (the row agent).}
    \label{tab:your_label}
    \begin{center}
    \begin{tabular}{lccccccccccccccccc|c}
    \toprule
    & \begin{sideways} mayari \end{sideways} & \begin{sideways} 2L \end{sideways} 
    & \begin{sideways} \textbf{\agentName} \end{sideways} & \begin{sideways} ObiBotKenobi \end{sideways} 
    & \begin{sideways} Aggrobot \end{sideways} & \begin{sideways} sophia \end{sideways} 
    & \begin{sideways} bRHEAdBot \end{sideways} & \begin{sideways} Ragnar \end{sideways} 
    & \begin{sideways} POLightRush \end{sideways} & \begin{sideways} SaveTheBeesV4 \end{sideways} 
    & \begin{sideways} POWorkerRush \end{sideways} & \begin{sideways} MyMicroRtsBot \end{sideways} 
    & \begin{sideways} NaiveMCTS \end{sideways} & \begin{sideways} myBot \end{sideways} 
    & \begin{sideways} NIlSiBot \end{sideways} & \begin{sideways} Predator \end{sideways} 
    & \begin{sideways} RandomBiasedAI \end{sideways} & \begin{sideways} Overall \end{sideways} \\
    \midrule
    mayari         & -      & 53 & 32          & 73           & 78       & 93     & 95        & 64     & 88          & 93            & 75           & 78            & 100       & 100   & 100      & 100      & 100            & 82      \\
    2L             & 51     & -  & 39          & 50           & 69       & 63     & 93        & 56     & 75          & 98            & 88           & 81            & 76        & 94    & 94       & 95       & 96             & 76      \\
    \textbf{\agentName}    & 62     & 59 & -           & 49           & 64       & 71     & 64        & 64     & 64          & 78            & 78           & 76            & 84        & 94    & 73       & 87       & 87             & 72      \\
    ObiBotKenobi   & 39     & 29 & 47          & -            & 47       & 69     & 60        & 56     & 58          & 83            & 65           & 76            & 72        & 99    & 79       & 85       & 100            & 66      \\
    Aggrobot       & 9      & 25 & 26          & 60           & -        & 69     & 55        & 44     & 63          & 86            & 69           & 94            & 66        & 94    & 94       & 91       & 94             & 65      \\
    sophia         & 25     & 44 & 30          & 35           & 38       & -      & 41        & 88     & 75          & 76            & 63           & 69            & 71        & 100   & 75       & 84       & 83             & 62      \\
    bRHEAdBot      & 4      & 7  & 24          & 44           & 49       & 69     & -         & 51     & 64          & 79            & 59           & 65            & 83        & 99    & 81       & 96       & 98             & 61      \\
    Ragnar         & 40     & 50 & 32          & 26           & 50       & 13     & 46        & -      & 44          & 71            & 63           & 69            & 73        & 88    & 81       & 73       & 85             & 56      \\
    POLightRush    & 0      & 25 & 29          & 38           & 31       & 44     & 34        & 38     & -           & 71            & 69           & 69            & 73        & 100   & 75       & 91       & 100            & 55      \\
    SaveTheBeesV4  & 14     & 9  & 21          & 43           & 31       & 59     & 38        & 47     & 66          & -             & 50           & 57            & 81        & 86    & 85       & 90       & 93             & 54      \\
    POWorkerRush   & 13     & 13 & 21          & 29           & 31       & 44     & 44        & 56     & 38          & 89            & -            & 75            & 49        & 94    & 81       & 81       & 96             & 53      \\
    MyMicroRtsBot  & 11     & 13 & 15          & 25           & 38       & 56     & 38        & 56     & 38          & 86            & 44           & -             & 43        & 94    & 69       & 74       & 92             & 49      \\
    NaiveMCTS      & 0      & 11 & 17          & 22           & 34       & 27     & 15        & 26     & 29          & 69            & 56           & 58            & -         & 92    & 46       & 60       & 84             & 40      \\
    myBot          & 1      & 6  & 21          & 20           & 39       & 48     & 28        & 41     & 43          & 77            & 39           & 40            & 50        & -     & 55       & 66       & 66             & 40      \\
    NIlSiBot       & 0      & 13 & 18          & 18           & 31       & 25     & 13        & 13     & 31          & 63            & 31           & 38            & 51        & 81    & -        & 58       & 73             & 35      \\
    Predator       & 1      & 7  & 13          & 6            & 12       & 21     & 11        & 16     & 14          & 56            & 22           & 28            & 44        & 73    & 43       & -        & 45             & 26      \\
    RandomBiasedAI & 0      & 1  & 15          & 0            & 4        & 15     & 6
    & 9      & 4           & 52            & 4            & 13            & 18        &
    85    & 39       & 39       & - & 19 \\
    \bottomrule
    \end{tabular}
\end{center}
\end{table}
\end{document}