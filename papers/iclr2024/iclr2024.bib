@article{DBLP:journals/corr/MnihKSGAWR13,
  author     = {Volodymyr Mnih and
                Koray Kavukcuoglu and
                David Silver and
                Alex Graves and
                Ioannis Antonoglou and
                Daan Wierstra and
                Martin A. Riedmiller},
  title      = {Playing Atari with Deep Reinforcement Learning},
  journal    = {CoRR},
  volume     = {abs/1312.5602},
  year       = {2013},
  url        = {http://arxiv.org/abs/1312.5602},
  eprinttype = {arXiv},
  eprint     = {1312.5602},
  timestamp  = {Mon, 13 Aug 2018 16:47:42 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/MnihKSGAWR13.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{Ontañón_Barriga_Silva_Moraes_Lelis_2018,
  title        = {The First microRTS Artificial Intelligence Competition},
  volume       = {39},
  url          = {https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/2777},
  doi          = {10.1609/aimag.v39i1.2777},
  abstractnote = {This article presents the results of the first edition of the microRTS (μRTS) AI competition, which was hosted by the IEEE Computational Intelligence in Games (CIG) 2017 conference. The goal of the competition is to spur research on AI techniques for real-time strategy (RTS) games. In this first edition, the competition received three submissions, focusing on address- ing problems such as balancing long-term and short-term search, the use of machine learning to learn how to play against certain opponents, and finally, dealing with partial observability in RTS games.},
  number       = {1},
  journal      = {AI Magazine},
  author       = {Ontañón, Santiago and Barriga, Nicolas A. and Silva, Cleyton R. and Moraes, Rubens O. and Lelis, Levi H. S.},
  year         = {2018},
  month        = {Mar.},
  pages        = {75-83}
}

@inproceedings{DBLP:journals/corr/LillicrapHPHETS15,
  author    = {Timothy P. Lillicrap and
               Jonathan J. Hunt and
               Alexander Pritzel and
               Nicolas Heess and
               Tom Erez and
               Yuval Tassa and
               David Silver and
               Daan Wierstra},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Continuous control with deep reinforcement learning},
  booktitle = {4th International Conference on Learning Representations, {ICLR} 2016,
               San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings},
  year      = {2016},
  url       = {http://arxiv.org/abs/1509.02971},
  timestamp = {Thu, 25 Jul 2019 14:25:37 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/LillicrapHPHETS15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Vinyals2019GrandmasterLI,
  title   = {Grandmaster level in StarCraft II using multi-agent reinforcement learning},
  author  = {Oriol Vinyals and Igor Babuschkin and Wojciech M. Czarnecki and Micha{\"e}l Mathieu and Andrew Dudzik and Junyoung Chung and David H. Choi and Richard Powell and Timo Ewalds and Petko Georgiev and Junhyuk Oh and Dan Horgan and Manuel Kroiss and Ivo Danihelka and Aja Huang and L. Sifre and Trevor Cai and John P. Agapiou and Max Jaderberg and Alexander Sasha Vezhnevets and R{\'e}mi Leblond and Tobias Pohlen and Valentin Dalibard and David Budden and Yury Sulsky and James Molloy and Tom Le Paine and Caglar Gulcehre and Ziyun Wang and Tobias Pfaff and Yuhuai Wu and Roman Ring and Dani Yogatama and Dario W{\"u}nsch and Katrina McKinney and Oliver Smith and Tom Schaul and Timothy P. Lillicrap and Koray Kavukcuoglu and Demis Hassabis and Chris Apps and David Silver},
  journal = {Nature},
  year    = {2019},
  pages   = {1-5},
  url     = {https://api.semanticscholar.org/CorpusID:204972004}
}

@article{Ontan2013TheCM,
  title   = {The Combinatorial Multi-Armed Bandit Problem and Its Application to Real-Time Strategy Games},
  author  = {Santiago Onta{\~n}{\'o}n},
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment},
  year    = {2013},
  url     = {https://api.semanticscholar.org/CorpusID:7281055}
}

@article{DBLP:journals/corr/abs-2105-13807,
  author     = {Shengyi Huang and
                Santiago Onta{\~{n}}{\'{o}}n and
                Chris Bamford and
                Lukasz Grela},
  title      = {Gym-{\(\mu\)}RTS: Toward Affordable Full Game Real-time Strategy Games
                Research with Deep Reinforcement Learning},
  journal    = {CoRR},
  volume     = {abs/2105.13807},
  year       = {2021},
  url        = {https://arxiv.org/abs/2105.13807},
  eprinttype = {arXiv},
  eprint     = {2105.13807},
  timestamp  = {Tue, 22 Jun 2021 11:44:11 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2105-13807.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/SchulmanWDRK17,
  author     = {John Schulman and
                Filip Wolski and
                Prafulla Dhariwal and
                Alec Radford and
                Oleg Klimov},
  title      = {Proximal Policy Optimization Algorithms},
  journal    = {CoRR},
  volume     = {abs/1707.06347},
  year       = {2017},
  url        = {http://arxiv.org/abs/1707.06347},
  eprinttype = {arXiv},
  eprint     = {1707.06347},
  timestamp  = {Mon, 13 Aug 2018 16:47:34 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/SchulmanWDRK17.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@misc{medeiros2022learn,
  title         = {What can we Learn Even From the Weakest? Learning Sketches for Programmatic Strategies},
  author        = {Leandro C. Medeiros and David S. Aleixo and Levi H. S. Lelis},
  year          = {2022},
  eprint        = {2203.11912},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AI}
}

@misc{zhuang2023behavior,
  title         = {Behavior Proximal Policy Optimization},
  author        = {Zifeng Zhuang and Kun Lei and Jinxin Liu and Donglin Wang and Yilang Guo},
  year          = {2023},
  eprint        = {2302.11312},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@inproceedings{pmlr-v97-han19a,
  title     = {Grid-Wise Control for Multi-Agent Reinforcement Learning in Video Game {AI}},
  author    = {Han, Lei and Sun, Peng and Du, Yali and Xiong, Jiechao and Wang, Qing and Sun, Xinghai and Liu, Han and Zhang, Tong},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  pages     = {2576--2585},
  year      = {2019},
  editor    = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume    = {97},
  series    = {Proceedings of Machine Learning Research},
  month     = {09--15 Jun},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v97/han19a/han19a.pdf},
  url       = {https://proceedings.mlr.press/v97/han19a.html},
  abstract  = {We consider the problem of multi-agent reinforcement learning (MARL) in video game AI, where the agents are located in a spatial grid-world environment and the number of agents varies both within and across episodes. The challenge is to flexibly control an arbitrary number of agents while achieving effective collaboration. Existing MARL methods usually suffer from the trade-off between these two considerations. To address the issue, we propose a novel architecture that learns a spatial joint representation of all the agents and outputs grid-wise actions. Each agent will be controlled independently by taking the action from the grid it occupies. By viewing the state information as a grid feature map, we employ a convolutional encoder-decoder as the policy network. This architecture naturally promotes agent communication because of the large receptive field provided by the stacked convolutional layers. Moreover, the spatially shared convolutional parameters enable fast parallel exploration that the experiences discovered by one agent can be immediately transferred to others. The proposed method can be conveniently integrated with general reinforcement learning algorithms, e.g., PPO and Q-learning. We demonstrate the effectiveness of the proposed method in extensive challenging multi-agent tasks in StarCraft II.}
}

@misc{chen2023emergent,
  title         = {Emergent collective intelligence from massive-agent cooperation and competition},
  author        = {Hanmo Chen and Stone Tao and Jiaxin Chen and Weihan Shen and Xihui Li and Chenghui Yu and Sikai Cheng and Xiaolong Zhu and Xiu Li},
  year          = {2023},
  eprint        = {2301.01609},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AI}
}

@misc{lux-ai-2021,
  author    = {Stone Tao and Isabelle Pan and Bovard Doerschuk-Tiberi and Addison Howard},
  title     = {Lux AI},
  publisher = {Kaggle},
  year      = {2021},
  url       = {https://kaggle.com/competitions/lux-ai-2021}
}

@misc{lux-ai-2021-winner,
  author = {Pressman Isaiah and Kirwin Liam and Sturrock Robert},
  title  = {Kaggle Lux AI 2021},
  year   = {2021},
  url    = {https://github.com/IsaiahPressman/Kaggle\_Lux\_AI\_2021}
}

@misc{Ferdinand2021doublecone,
  author = {Ferdinand Limberg},
  title  = {FLG's Approach - Deep Reinforcement Learning with a Focus on Performance - 4th place},
  note   = {Kaggle discussion: \url{https://www.kaggle.com/competitions/lux-ai-season-2/discussion/406702}},
  year   = {2023}
}

@misc{Winter2021,
  author = {Clemens Winter},
  title  = {Mastering Real-Time Strategy Games with Deep Reinforcement Learning: Mere Mortal Edition},
  year   = {2021},
  url    = {https://clemenswinter.com/2021/03/24/mastering-real-time-strategy-games-with-deep-reinforcement-learning-mere-mortal-edition},
  note   = {Accessed: 2023-07-16}
}

@article{DBLP:journals/corr/abs-2006-14171,
  author     = {Shengyi Huang and
                Santiago Onta{\~{n}}{\'{o}}n},
  title      = {A Closer Look at Invalid Action Masking in Policy Gradient Algorithms},
  journal    = {CoRR},
  volume     = {abs/2006.14171},
  year       = {2020},
  url        = {https://arxiv.org/abs/2006.14171},
  eprinttype = {arXiv},
  eprint     = {2006.14171},
  timestamp  = {Wed, 01 Jul 2020 15:21:23 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2006-14171.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}