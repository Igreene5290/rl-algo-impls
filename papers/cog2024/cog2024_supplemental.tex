\documentclass[conference,onecolumn]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{url}
\usepackage{times}
\usepackage{array}
\usepackage{paralist}
\usepackage{float}
\usepackage[utf8]{inputenc}
\usepackage{threeparttable}
\usepackage{booktabs}
\usepackage[T1]{fontenc}
\usepackage{tabularx}
\usepackage{ragged2e}
\usepackage{adjustbox}
\usepackage{rotating}
\usepackage{subcaption}
\usepackage{colortbl} % for cellcolor
\usepackage{pgf}      % for pgfmathsetmacro

\definecolor{lightred}{RGB}{255, 204, 204}
\definecolor{lightblue}{RGB}{204, 204, 255}
\definecolor{white}{RGB}{255, 255, 255}

\newcommand{\colcellbuffer}{\rule{-0.33em}{2ex}}

\newcommand{\colcell}[1]{%
    \ifnum #1>50
        \pgfmathsetmacro{\redComponent}{2*(#1-50)}
        \edef\clrmacro{\noexpand\cellcolor{lightred!\redComponent!white}}\clrmacro{\colcellbuffer\textbf{#1}\colcellbuffer}
    \else
        \pgfmathsetmacro{\blueComponent}{2*(50-#1)}
        \edef\clrmacro{\noexpand\cellcolor{lightblue!\blueComponent!white}}\clrmacro{\colcellbuffer#1\colcellbuffer}
    \fi
}

\newcommand{\colcellnobold}[1]{%
    \ifnum #1>50
        \pgfmathsetmacro{\redComponent}{2*(#1-50)}
        \edef\clrmacro{\noexpand\cellcolor{lightred!\redComponent!white}}\clrmacro{\colcellbuffer#1\colcellbuffer}
    \else
        \pgfmathsetmacro{\blueComponent}{2*(50-#1)}
        \edef\clrmacro{\noexpand\cellcolor{lightblue!\blueComponent!white}}\clrmacro{\colcellbuffer#1\colcellbuffer}
    \fi
}

\newcommand{\colcellbold}[1]{%
    \ifnum #1>50
        \pgfmathsetmacro{\redComponent}{2*(#1-50)}
        \edef\clrmacro{\noexpand\cellcolor{lightred!\redComponent!white}}\clrmacro{\colcellbuffer\textbf{#1}\colcellbuffer}
    \else
        \pgfmathsetmacro{\blueComponent}{2*(50-#1)}
        \edef\clrmacro{\noexpand\cellcolor{lightblue!\blueComponent!white}}\clrmacro{\colcellbuffer\textbf{#1}\colcellbuffer}
    \fi
}

\usepackage{xr}
\externaldocument{cog2024}

\newcounter{suppfigure}
\renewcommand{\thesuppfigure}{S\arabic{suppfigure}}
\newenvironment{suppfigure}
  {\renewcommand{\figurename}{Supplemental Fig.}\setcounter{figure}{\value{suppfigure}}\addtocounter{suppfigure}{1}\begin{figure}}
  {\end{figure}\setcounter{suppfigure}{\value{figure}}}

\newcounter{supptable}
\renewcommand{\thesupptable}{S\arabic{supptable}}
\newenvironment{supptable}
  {\renewcommand{\tablename}{Supplemental Table}\setcounter{table}{\value{supptable}}\addtocounter{supptable}{1}\begin{table}}
  {\end{table}\setcounter{supptable}{\value{table}}}

\usepackage{cog2024_anonymization}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}


\newcommand{\mapname}[1]{#1} % Normal font
%\newcommand{\mapname}[1]{\texttt{#1}} % Uncomment to use texttt

\begin{document}

\title{A Competition Winning Deep Reinforcement Learning Agent in microRTS: supplemental document}
\author{}
\maketitle

\section{Neural network architecture}
DoubleCone(4, 6, 4) \cite{Ferdinand2021doublecone-supp} consists of
\begin{inparaenum}[(1)]
    \item 4 residual blocks;
    \item a downscaled residual block consisting of a stride-4 convolution, 6 residual blocks, and
        2 stride-2 transpose convolutions;
    \item 4 residual blocks; and
    \item actor and value heads (Supplemental Fig.~\ref{suppfig:doublecone}).
\end{inparaenum}
Each residual block includes a squeeze-excitation layer after the second convolutional
layer (Supplemental Fig.~\ref{fig:squeezeexcitation}). The values heads are each 
\begin{inparaenum}[(1)]
    \item 2 stride-2 convolutions,
    \item an adaptive average pooling layer,
    \item flattened,
    \item 2 densely connected layers, and
    \item an activation function (Identity [no activation] or Tanh) to a single, scalar
    value (Supplemental Fig.~\ref{fig:valueheads}).
\end{inparaenum}
The adaptive average pooling layer allows the network to be used on different map sizes.

\begin{suppfigure}[H]
    \begin{center}
        \includegraphics[width=0.5\linewidth]{figures/DoubleCone.png}
    \end{center}
    \caption{DoubleCone(4, 6, 4) neural network architecture.}
    \label{suppfig:doublecone}
\end{suppfigure}

\begin{suppfigure}[H]
    \begin{center}
        \includegraphics[width=0.3\linewidth]{figures/SqueezeExcitation.png}
    \end{center}
    \caption{ResBlock used in DoubleCone, squnet32, and squnet64. The residual block is similar to a standard residual block but inserts a Squeeze-Excitation block after the convolutional layers and before the residual connection.}
    \label{fig:squeezeexcitation}
\end{suppfigure}

\begin{suppfigure}[H]
    \begin{center}
        \includegraphics[width=0.7\linewidth]{figures/ValueHeads.png}
    \end{center}
    \caption{Value heads used in (from left to right) DoubleCone, squnet32, and
    squnet64. The AdaptiveAvgPool2d layer allows the network to be used on various map
    sizes.}
    \label{fig:valueheads}
\end{suppfigure}

\begin{suppfigure}[H]
    \begin{center}
        \includegraphics[width=0.45\linewidth]{figures/squnet64.png}
    \end{center}
    \caption{squnet64 neural network architecture. Instead of one downscaling block as in DoubleCone, this network downscales 3 times. This aggressive downscaling reduces the number of computations for larger maps, while theoretically supporting a large receptive field.}
    \label{fig:squnet}
\end{suppfigure}
        
\begin{supptable}[h]
    \centering
    \begin{threeparttable}
        \caption{Comparison of different architectures}
        \label{table:architectureBreakdown}
        \begin{tabular}{cccc}
            & \textbf{DoubleCone} & \textbf{squnet-map32\tnote{a}} & \textbf{squnet-map64} \\
            \midrule
            Levels & 2 & 4 & 4 \\
            Encoder residual blocks/level & [4, 6] & [1, 1, 1, 1] & [1, 1, 1, 1] \\
            Decoder residual blocks/level & [4] & [1, 1, 1] & [1, 1, 1] \\
            Stride/level & [4] & [2, 2, 4] & [2, 4, 4] \\
            Deconvolution strides/level & [[2, 2]\tnote{b}{ }] & [2, 2, 4] & [2, 4, 4] \\
            Channels/level & [128, 128] & [128, 128, 128, 128] & [64, 64, 64, 64] \\
            Trainable parameters & 5,014,865 & 3,584,657 & 1,420,625 \\
            MACs\tnote{c} & \begin{tabular}[c]{@{}c@{}}0.70B (16x16)\tnote{d} \\ 0.40B (12x12)\tnote{e} \\ 1.58B (24x24) \\ 2.81B (32x32)\end{tabular} & 1.16B (32x32) & 1.41B (64x64) \\ 
        \end{tabular}
        \begin{tablenotes}
            \item[a] Used by ppo-Microrts-squnet-DistantResources-128ch-finetuned-S1-best and ppo-Microrts-squnet-map32-128ch-selfplay-S1-best. 
            \item[b] 2 stride-2 transpose convolutions to match the 1 stride-4 convolution.
            \item[c] Multiply-Accumulates for computing actions for a single observation.
            \item[d] All maps smaller than 16x16 (except NoWhereToRun9x8) are padded with walls up to 16x16. 
            \item[e] NoWhereToRun9x8 is padded with walls up to 12x12.
        \end{tablenotes}
    \end{threeparttable}
\end{supptable}

\section{Initial training details}
\label{appendix:initial-training-details}
\agentName\ was trained with partial observability and environment non-determinism disabled.

\begin{supptable}[H]
    \centering
    \begin{threeparttable}
    \caption{Initial training schedule from a randomly initialized model}
    \label{tab:initial-training-schedule}
    \begin{tabular}{lccccr}
    \toprule
     & Phase 1 & Transition 1→2\tnote{a} & Phase 2 & Transition 2→3\tnote{a} & Phase 3 \\
    \midrule
    steps & 90M & 60M & 30M & 60M & 60M \\
    reward weights\tnote{b} & [0.8, 0.01, 0.19] &  & [0, 0.5, 0.5] &  & [0, 0.99, 0.01] \\
    $c_1$ (value loss coef)\tnote{b} & [0.5, 0.1, 0.2] &  & [0, 0.4, 0.4] &  & [0, 0.5, 0.1]\\
    $c_2$ (entropy coef)& 0.01 & & 0.01 & & 0.001 \\
    learning rate & $10^{-4}$ & & $10^{-4}$ & & $5 \times 10^{-5}$\\
    \bottomrule
    \end{tabular}
    \begin{tablenotes}
       \item[a] Values are linearly interpolated between phases based on step count.
       \item[b] Listed weights are for the shaped, win-loss, cost-based values, respectively.
    \end{tablenotes}
    \end{threeparttable}
\end{supptable}

\begin{supptable}[H]
    \centering
    \begin{threeparttable}
    \caption{Comparison of initial training, shaped fine-tuning, and sparse fine-tuning parameters}
    \label{tab:training-parameters}
    \begin{tabular}{lccc}
    Parameter & Initial Training & Shaped Fine-Tuning & Sparse Fine-Tuning \\
    \midrule
    Steps & 300M &   100M &   100M \\
    Number of Environments & 24 &    \textquotedbl &    \textquotedbl \\
    Rollout Steps Per Env & 512 &   \textquotedbl &   \textquotedbl \\
    Minibatch Size & 4096 &   \textquotedbl &   \textquotedbl \\
    Epochs Per Rollout & 2 &   \textquotedbl &   \textquotedbl \\
    $\gamma$ (Discount Factor) & [0.99, 0.999, 0.999]\tnote{a} &   \textquotedbl &   \textquotedbl \\
    GAE $\lambda$ & [0.95, 0.99, 0.99]\tnote{a} &   \textquotedbl &   \textquotedbl \\
    Clip Range & 0.1 &   \textquotedbl &   \textquotedbl \\
    Clip Range VF & 0.1 &    \textquotedbl &   \textquotedbl \\
    VF Coef Halving\tnote{b} & True  &    \textquotedbl &   \textquotedbl \\
    Max Grad Norm &  0.5 &   \textquotedbl &   \textquotedbl \\   
    Latest Self-play Envs   &   12 &                      \textquotedbl &                      \textquotedbl \\
    Old Self-play Envs   &   12 &                      0 &                      0 \\
    Bots   &   none & CoacAI: 12 & \begin{tabular}[c]{@{}c}CoacAI: 6\\ Mayari: 6\end{tabular} \\
    Maps   &   \begin{tabular}[c]{@{}c}basesWorkers16x16A \\ TwoBasesBarracks16x16 \\
    basesWorkers8x8A \\ FourBasesWorkers8x8 \\ NoWhereToRun9x8 \\
    EightBasesWorkers16x16\tnote{c} \end{tabular} &  \textquotedbl & \textquotedbl \\
    \end{tabular}
    \begin{tablenotes}
    \item[\textquotedbl] Same value as cell to left.
    \item[a] Value per value head (shaped, win-loss, cost-based).
    \item[b] Multiply $v_{\text{loss}}$ by 0.5, as done in CleanRL.
    \item[c] Map not used in competition.
    \end{tablenotes}
    \end{threeparttable}
\end{supptable}

\begin{supptable}[H]
    \centering
    \begin{threeparttable}
    \caption{Shaped fine-tuning schedule}
    \label{tab:shaped-finetuning-schedule}
    \begin{tabular}{lccccc}
    \toprule
     & Start & Transition →1\tnote{a} & Phase 1 & Transition 1→2\tnote{a} & Phase 2 \\
     \midrule
    steps & & 5M & 30M & 20M & 45M \\
    reward weights\tnote{b} & [0, 0.99, 0.01] & & [0, 0.5, 0.5] & & [0, 0.99, 0.01] \\
    $c_1$ (value loss coef)\tnote{b} & [0, 0.4, 0.2] & & [0, 0.4, 0.4] & & [0, 0.5, 0.1]\\
    $c_2$ (entropy coef) & 0.01 & & 0.01 & & 0.001 \\
    learning rate & $10^{-5}$ & & $5 \times 10^{-5}$ & & $5 \times 10^{-5}$\\
    \bottomrule
    \end{tabular}
    \begin{tablenotes}
       \item[a] Values are linearly interpolated between phases based on step count.
       \item[b] Listed weights are for the shaped, win-loss, cost-based values, respectively.
    \end{tablenotes}
    \end{threeparttable}
\end{supptable}

\begin{supptable}[H]
    \centering
    \begin{threeparttable}
    \caption{Sparse fine-tuning schedule}
    \label{tab:sparse-finetuning-schedule}
    \begin{tabular}{lcccc}
    \toprule
     & Phase 1 & Transition 1→2\tnote{a} & Phase 2 \\
     \midrule
    steps & 30M & 40M & 30M \\
    reward weights\tnote{b} & [0, 0.99, 0.01] &  & [0, 0.99, 0.01] \\
    $c_1$ (value loss coef)\tnote{b} & [0, 0.5, 0.1] &  & [0, 0.5, 0.1]\\
    $c_2$ (entropy coef)) & 0.001 & & 0.0001 \\
    learning rate & $5 \times 10^{-5}$ & & $10^{-5}$ \\
    \bottomrule
    \end{tabular}
    \begin{tablenotes}
       \item[a] Values are linearly interpolated between phases based on step count.
       \item[b] Listed weights are for the shaped, win-loss, cost-based values, respectively.
    \end{tablenotes}
    \end{threeparttable}
\end{supptable}

\section{Transfer learning details}
\label{appendix:transfer-learning-details}

\begin{supptable}[H]
    \centering
    \begin{threeparttable}
    \caption{Transfer learning schedule starting from ppo-Microrts-A6000-finetuned-coac-mayari-S1-best model}
    \label{tab:transfer-learning-schedule}
    \begin{tabular}{lccccc}
    \toprule
     & Start & Transition →1\tnote{a} & Phase 1 & Transition 1→2\tnote{a} & Phase 2 \\
     \midrule
    steps & & 5M & 30M & 20M & 45M \\
    reward weights\tnote{b} & [0, 0.99, 0.01] & & [0.4, 0.5, 0.1] & & [0, 0.99, 0.01] \\
    $c_1$ (value loss coef)\tnote{b} & [0.2, 0.4, 0.2] & & [0.3, 0.4, 0.1] & & [0, 0.5, 0.1]\\
    $c_2$ (entropy coef) & 0.01 & & 0.01 & & 0.0001 \\
    learning rate & $5 \times 10^{-5}$ & & $7 \times 10^{-5}$ & & $10^{-5}$ \\
    \bottomrule
    \end{tabular}
    \begin{tablenotes}
       \item[a] Values are linearly interpolated between phases based on step count.
       \item[b] Listed weights are for the shaped, win-loss, cost-based values, respectively.
    \end{tablenotes}
    \end{threeparttable}
\end{supptable}

\section{squnet learning details}
\label{appendix:squnet-learning-details}

\begin{supptable}[H]
    \centering
    \begin{threeparttable}
    \caption{Squnet training parameters}
    \label{tab:squnet-training-parameters}
    \begin{tabular}{lccc}
    \toprule
    Parameter & map32 & map32-DistantResources & map64 \\
    \midrule
    Steps & 200M & 100M & 200M \\
    n\_envs & 24 & \textquotedbl & \textquotedbl \\
    Rollout Steps Per Env & 512 & 512 & 256 \\
    Minibatch Size & 2048 & 2048 & 258 \\
    Clip Range & 0.1 & \textquotedbl & \textquotedbl \\
    Clip Range VF & none & \textquotedbl & \textquotedbl \\
    Latest Self-play Envs & 12 & \textquotedbl & \textquotedbl \\
    Old Self-play Envs & 6 & 6 & 4 \\
    Bots & \begin{tabular}[c]{@{}c}CoacAI: 3 \\ Mayari: 3 \end{tabular} & \begin{tabular}[c]{@{}c}CoacAI: 3 \\ Mayari: 3 \end{tabular} & \begin{tabular}[c]{@{}c}CoacAI: 4 \\ Mayari: 4 \end{tabular} \\
    Maps & \begin{tabular}[c]{@{}c}DoubleGame24x24 \\ BWDistantResources32x32 \\ chambers32x32\tnote{a} \end{tabular} & BWDistantResources32x32 & \begin{tabular}[c]{@{}c}BloodBath.scmB \\ BloodBath.scmE\tnote{a}\end{tabular} \\   
    \bottomrule
    \end{tabular}
    \begin{tablenotes}
    \item[\textquotedbl] Same value as cell to left.
    \item[a] Not competition Open maps.
    \end{tablenotes}
    \end{threeparttable}
\end{supptable}

\begin{supptable}[H]
    \centering
    \begin{threeparttable}
    \caption{squnet training schedule starting with randomly initialized weights}
    \label{tab:squnet-training-schedule}
    \begin{tabular}{lcccc}
    \toprule
     & Phase 1 & Transition 1→2\tnote{a} & Phase 2 \\
     \midrule
    steps & 100M & 60M & 40M \\
    reward weights\tnote{b} & [0.8, 0.01, 0.19] &  & [0, 0.99, 0.01] \\
    $c_1$ (value loss coef)\tnote{b} & [0.5, 0.1, 0.2] &  & [0, 0.5, 0.1]\\
    $c_2$ (entropy coef) & 0.01 & & 0.001 \\
    learning rate & $10^{-4}$ & & $5 \times 10^{-5}$ \\
    \bottomrule
    \end{tabular}
    \begin{tablenotes}
       \item[a] Values are linearly interpolated between phases based on step count.
       \item[b] Listed weights are for the shaped, win-loss, cost-based values, respectively.
    \end{tablenotes}
    \end{threeparttable}
\end{supptable}

\section{Behavior cloning details}
\label{appendix:behavior-cloning-details}
\begin{supptable}[H]
    \centering
    \begin{threeparttable}
    \caption{Neural architecture for behavior cloning and PPO fine-tuned training}
    \label{tab:bc-architecture}
    \begin{tabular}{lc}
    \toprule
                                 & deep16-128 \\
    \midrule
    Levels                      & 3  \\
    Encoder residual blocks/level & [3, 2, 4] \\
    Decoder residual blocks/level & [3, 2] \\
    Stride per level            & [4, 4] \\
    Deconvolution strides per level & [[2, 2], [2, 2]]\tnote{a}\\
    Channels per level          & [128, 128, 128] \\
    Trainable parameters        & 5,027,279 \\
    MACs\tnote{b} (16x16)          & 0.52B \\
    MACs\tnote{b} (64x64)          & 8.40B \\
    \bottomrule
    \end{tabular}
    \begin{tablenotes}
       \item[a] 2 stride-2 transpose convolutions to match the 1 stride-4 convolution.
       \item[b] Multiply-Accumulates for computing actions for a single observation.
    \end{tablenotes}
    \end{threeparttable}
\end{supptable}

\begin{supptable}[H]
    \centering
    \begin{threeparttable}
    \caption{Behavior cloning training parameters}
    \label{tab:bc-training-parameters}
    \begin{tabular}{lccc}
    Map Size & 16x16 & 32x32 & 64x64 \\
    \midrule
    Steps & 100M & \textquotedbl & \textquotedbl \\
    Number of Environments & 36 & 24 & 24 \\
    Rollout Steps Per Env & 512 & \textquotedbl & \textquotedbl \\
    Minibatch Size & 3072 & 768 & 192 \\
    Epochs Per Rollout & 2 & \textquotedbl & \textquotedbl \\
    $\gamma$ (Discount Factor) & 0.999 & 0.9996 & 0.999 \\
    GAE $\lambda$ & 0.99 & 0.996 & 0.999 \\
    Max Grad Norm & 0.5 & \textquotedbl & \textquotedbl \\
    Gradient Accumulation & FALSE & FALSE & TRUE \\
    Scale Loss by \# Actions & TRUE & \textquotedbl & \textquotedbl \\
    \hline
    Bots & \begin{tabular}[c]{@{}c@{}}Mayari: 12\\ CoacAI: 12\\ POLightRush:
    12\end{tabular} & \begin{tabular}[c]{@{}c@{}}Mayari: 12\\ CoacAI: 6\\ POLightRush:
    6\end{tabular} & \begin{tabular}[c]{@{}c@{}}Mayari: 8\\ CoacAI: 8\\ POLightRush:
    8\end{tabular} \\
    \hline
    Maps & \begin{tabular}[c]{@{}c@{}}basesWorkers16x16A\\ TwoBasesBarracks16x16\\
    basesWorkers8x8A\\ FourBasesWorkers8x8\\ NoWhereToRun9x8\\
    EightBasesWorkers16x16\end{tabular} & \begin{tabular}[c]{@{}c@{}}DoubleGame24x24\\
    BWDistantResources32x32\\ chambers32x32\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}(4)BloodBath.scmB\\ (4)BloodBath.scmE\end{tabular}
    \end{tabular}
    \begin{tablenotes}
    \item[\textquotedbl] Same value as cell to left.
    \end{tablenotes}
\end{threeparttable}
\end{supptable}

\begin{supptable}[H]
    \centering
    \begin{threeparttable}
    \caption{Training parameters for PPO of behavior cloned models}
    \label{tab:bc-ppo-training-parameters}
    \begin{tabular}{lccc}
    Map Size & 16x16 & 32x32 & 64x64 \\
    \midrule
    Steps & 100M & 200M & 200M \\
    Number of Environments & 36 & 24 & 48 \\
    Rollout Steps Per Env & 512 & \textquotedbl & \textquotedbl \\
    Minibatch Size & 3072 & 768 & 192 \\
    Epochs Per Rollout & 2 & \textquotedbl & \textquotedbl \\
    {\color[HTML]{1A1A1A} $\gamma$ (Discount Factor)} & 0.999 & 0.9996 & 0.99983 \\
    GAE $\lambda$ & 0.99 & 0.996 & 0.9983 \\
    Clip Range & 0.1 & \textquotedbl & \textquotedbl \\
    Clip Range VF & none & \textquotedbl & \textquotedbl \\
    VF Coef Halving\tnote{a} & TRUE & \textquotedbl & \textquotedbl \\
    Max Grad Norm & 0.5 & \textquotedbl & \textquotedbl \\
    Gradient Accumulation & FALSE & TRUE & TRUE \\
    Latest Selfplay Envs & 12 & 12 & 28 \\
    Old Selfplay Envs & 12 & 6 & 12 \\
    Bots & \begin{tabular}[c]{@{}c@{}}Mayari: 6\\ CoacAI: 6\end{tabular} & \begin{tabular}[c]{@{}c@{}}Mayari: 3\\ CoacAI: 3\end{tabular} & \begin{tabular}[c]{@{}c@{}}Mayari: 2\\ CoacAI: 2\\ POLightRush: 2\\ POWorkerRush: 2\end{tabular} \\
    Maps & \begin{tabular}[c]{@{}c@{}}basesWorkers16x16A\\ TwoBasesBarracks16x16\\ basesWorkers8x8A\\ FourBasesWorkers8x8\\ NoWhereToRun9x8\\ EightBasesWorkers16x16\end{tabular} & \begin{tabular}[c]{@{}c@{}}DoubleGame24x24\\ BWDistantResources32x32\\ chambers32x32\end{tabular} & \begin{tabular}[c]{@{}c@{}}(4)BloodBath.scmB\\ (4)BloodBath.scmE\end{tabular}
    \end{tabular}
    \begin{tablenotes}
    \item[\textquotedbl] Same value as cell to left.
    \item[a] Multiply $v_{\text{loss}}$ by 0.5, as done in CleanRL.
    \end{tablenotes}
\end{threeparttable}
\end{supptable}

\begin{supptable}[H]
    \caption{Behavior cloning schedule for 16x16 maps. Values in transition are linearly interpolated.}
    \label{tab:bc-schedule-map16}
    \begin{center}
    \begin{tabular}{lccc}
    & Start & Transition & End \\
    \midrule
    & & 100M & \\
    learning rate & $8 \times 10^{-5}$ &  & 0 \\
    \end{tabular}
\end{center}
\end{supptable}

\begin{supptable}[H]
    \caption{Behavior cloning schedule for 32x32 and 64x64 maps. Values in transitions are cosine interpolated.}
    \label{tab:bc-schedule}
    \begin{center}
    \begin{tabular}{lccccc}
    & Start & Transition →1 & Phase 1 & Transition 1→2 & Phase 2 \\
    \midrule
    &  & 5M & 5M & 85M & 5M \\
    learning rate & $10^{-5}$ &  & $8 \times 10^{-5}$ &  & $10^{-6}$
    \end{tabular}
\end{center}
\end{supptable}

\begin{supptable}[H]
    \caption{Schedule for PPO fine-tuning of behavior cloned model for 16x16 map. Transition values are cosine interpolated.}
    \label{tab:bc-ppo-schedule-map16}
    \begin{center}
    \begin{tabular}{lccccc}
    & Start & Transition →1 & Phase 1 & Transition 1→2 & Phase 2 \\
    \midrule
    &  & 5M & 5M & 85M & 5M \\
    $c_2$ (entropy coef) & 0.001 &  & 0.001 &  & 0.0001 \\
    learning rate & $10^{-5}$ &  & $5 \times 10^{-5}$ &  & $10^{-5}$ \\
    \end{tabular}
\end{center}
\end{supptable}

\begin{supptable}[H]
    \caption{Schedule for PPO fine-tuning of behavior cloned model for 32x32 map. Transition values are cosine interpolated.}
    \label{tab:bc-ppo-schedule-map32}
    \begin{center}
    \begin{tabular}{lccccc}
    & Start & Transition →1 & Phase 1 & Transition 1→2 & Phase 2 \\
    \midrule
    &  & 10M & 80M & 70M & 40M \\
    $c_2$ (entropy coef) & 0.001 &  & 0.001 &  & 0.0001 \\
    learning rate & $10^{-5}$ &  & $5 \times 10^{-5}$ &  & $10^{-5}$ \\
    \end{tabular}
\end{center}
\end{supptable}

\begin{supptable}[H]
    \caption{Schedule for PPO fine-tuning of behavior cloned model for 64x64 map. Transition values are cosine interpolated. Transition 1→2 being empty means values jump from Phase 1 to Phase 2.}
    \label{tab:bc-ppo-schedule-map64}
    \begin{center}
        \begin{tabular}{lccccccccc}
            & \begin{sideways} Start \end{sideways} & \begin{sideways} Transition →1
            \end{sideways} & \begin{sideways} Phase 1 \end{sideways} & \begin{sideways}
            Transition 1→2 \end{sideways} & \begin{sideways} Phase 2 \end{sideways} &
            \begin{sideways} Transition 2→3 \end{sideways} & \begin{sideways} Phase 3
            \end{sideways} & \begin{sideways} Transition 3→4 \end{sideways} &
            \begin{sideways} Phase 4 \end{sideways} \\
            \midrule
            &  & 10M &  &  &  & 40M & 80M & 66M & 4M \\
           $c_2$ (entropy coef) & 0 &  & 0 &  & 0.001 &  & 0.001 &  & 0.0001 \\
           learning rate & $10^{-6}$ &  & $5 \times 10^{-5}$ & & $10^{-6}$ &  & $5 \times 10^{-5}$ &  & $10^{-6}$ \\
           \begin{tabular}[c]{@{}l@{}}freeze backbone \\ and policy head\end{tabular} & TRUE &  & TRUE &  & FALSE &  & FALSE &  & FALSE
    \end{tabular}
\end{center}
\end{supptable}

\section{Training durations}
We trained using Lambda Labs GPU on-demand instances. We used single Nvidia GPU instances, but
different GPUs to be able to fit larger minibatches onto the GPU. A10 (24 GB VRAM) and
A100 (40 GB VRAM) machines had 30 vCPUs and 200 GB RAM. A6000 (48 GB VRAM) machines had
14 vCPUs and 100 GB RAM. We did not fully utilize the CPU, RAM, GPU compute, or hard drive resources
during training.

Behavior cloning and PPO fine-tuning of behavior cloned models were trained only using
A10 machines. We had implemented gradient accumulation at this point to support larger
batch sizes that did not need to fit on the GPU all-at-once.

\label{appendix:training-durations}
\begin{supptable}[H]
    \caption{\agentName\ training durations. Blank models are intermediate models that
    lead to the next row. For example, the first 3 runs are intermediate models for
    16x16. Runs are uploaded to the \wbProject, except
    for squnet-DistantResources (\microRTSWBProjectPath).}
    \label{tab:training-durations}
    \begin{center}
    \begin{tabular}{lccc}
        \multicolumn{1}{c}{Map} & Run ID & GPU & Days Training \\
        \midrule
         & \texttt{df4flrs4} & A10 & 12.5 \\
         & \texttt{9bz7wsuv} & A6000 & 2.7 \\
         & \texttt{tff7xk4b} & A6000 & 4.1 \\
        \multicolumn{1}{c}{16x16} & \texttt{1ilo9yae} & A6000 & 4.3 \\
         \hline
         & \texttt{hpp5pffx} & A10 & 1.9 \\
        \multicolumn{1}{c}{NoWhereToRun9x8} & \texttt{vmns9sbe} & A10 & 1.7 \\
         \hline
        \multicolumn{1}{c}{DoubleGame24x24} & \texttt{unnxtprk} & A6000 & 5.3 \\
         \hline
        \multicolumn{1}{c}{BWDistantResources32x32} & \texttt{x4tg80vk} & A100 & 3.6 \\
         \hline
        \multicolumn{1}{c}{32x32} & \texttt{tga53t25} & A6000 & 10.2 \\
        \multicolumn{1}{c}{squnet-DistantResources} & \texttt{jl8zkpfr} & A6000 & 5.0 \\
         \hline
        \multicolumn{1}{c}{64x64} & \texttt{nh5pdv4o} & A6000 & 19.0 \\
        \hline
         & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & 70.4 \\
         & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{}
        \end{tabular}
\end{center}
\end{supptable}

\begin{supptable}[H]
    \caption{Behavior cloning training durations. Runs are uploaded to the \microRTSWBProject.}
    \label{tab:bc-training-durations}
    \begin{center}
    \begin{tabular}{llc}
        Map Size & Run ID & Days Training \\
        \midrule
        16x16 & \texttt{lhs1b2gj} & 3.5 \\
        32x32 & \texttt{16o4391r} & 4.7 \\
        64x64 & \texttt{uksp6znl} & 15.1 \\
        \hline
        \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & 23.3
    \end{tabular}
\end{center}
\end{supptable}

\begin{supptable}[H]
    \caption{Training durations for PPO fine-tuning of behavior cloned models. Runs are uploaded to the \microRTSWBProject.}
    \label{tab:bc-ppo-training-durations}
    \begin{center}
    \begin{tabular}{llc}
        Map Size & Run ID & Days Training \\
        \midrule
        16x16 & \texttt{a4efzeug} & 4.0 \\
        32x32 & \texttt{042rwd8p} & 11.3 \\
        64x64 & \texttt{9l2debnz} & 33.9 \\
        \hline
        \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & 49.1
    \end{tabular}
\end{center}
\end{supptable}

\section{Single player round-robin benchmark setup}
\label{appendix:single-player-benchmark-setup}
In Section~\ref{sec:single-player-benchmark}, \agentName\ plays on the 8 Open maps
against 4 opponents:
\begin{inparaenum}[(1)]
    \item baseline POWorkerRush,
    \item baseline and 2017 competition winner POLightRush,
    \item 2020 competition winner CoacAI, and
    \item last competition (2021) winner Mayari.
\end{inparaenum}
\agentName\ normally plays against each opponent on each map for 100 matches (50
each as player 1 and 2). The exception is the squnet model for \mapname{BWDistantResources32x32},
where \agentName\ only played each opponent for 20 matches (10 each as player 1 and 2).
All opponents use A* for pathfinding, which is default for competitions.  Win rates are 
percentages of wins where draws count as 0.5 wins for each player. The single player 
round-robin benchmark was run on a 2018 Mac Mini with Intel i7-8700B CPU (6-core, 
3.2GHz) with PyTorch limited to 6 threads. Timeouts were set to 100 ms. If
an agent took 20ms over the deadline (120 ms total), the game was terminated and the win
awarded to the opponent.

In Section~\ref{sec:behavior-cloning-results}, \bcAgent\ and \bcPPOAgent\ play each
opponent on each map for 20 matches
(10 each as player 1 and 2).

\section{Additional IEEE-CoG 2023 microRTS competition details}
\begin{supptable}[H]
    \caption{Win rates of all agents in the IEEE-CoG 2023 microRTS competition on Open maps. Player 1 is the row agent and player 2 is the column agent. Each win rate value is the percentage of games won by player 1 (the row agent). Higher win rates are redder. Lower win rates are bluer.}
    \label{tab:all-competition-winrate}
    \begin{center}
    \arrayrulecolor{black}
    \begin{tabular}{lccccccccccccccccc|c}
    & \begin{sideways} Mayari \end{sideways} & \begin{sideways} 2L \end{sideways} 
    & \begin{sideways} \textbf{\agentName} \end{sideways} & \begin{sideways} ObiBotKenobi \end{sideways} 
    & \begin{sideways} Aggrobot \end{sideways} & \begin{sideways} sophia \end{sideways} 
    & \begin{sideways} bRHEAdBot \end{sideways} & \begin{sideways} Ragnar \end{sideways} 
    & \begin{sideways} POLightRush \end{sideways} & \begin{sideways} SaveTheBeesV4 \end{sideways} 
    & \begin{sideways} POWorkerRush \end{sideways} & \begin{sideways} MyMicroRtsBot \end{sideways} 
    & \begin{sideways} NaiveMCTS \end{sideways} & \begin{sideways} myBot \end{sideways} 
    & \begin{sideways} NIlSiBot \end{sideways} & \begin{sideways} Predator \end{sideways} 
    & \begin{sideways} RandomBiasedAI \end{sideways} & \begin{sideways} Overall \end{sideways} \\
    \arrayrulecolor{black}\specialrule{.5pt}{0pt}{0pt}
    Mayari         & -      & \colcellnobold{53} & \colcellnobold{32} & \colcellnobold{73} & \colcellnobold{78} & \colcellnobold{93} & \colcellnobold{95} & \colcellnobold{64} & \colcellnobold{88} & \colcellnobold{93} & \colcellnobold{75} & \colcellnobold{78} & \colcellnobold{100} & \colcellnobold{100} & \colcellnobold{100} & \colcellnobold{100} & \colcellnobold{100} & \colcellnobold{82} \\
    2L             & \colcellnobold{51} & -  & \colcellnobold{39} & \colcellnobold{50} & \colcellnobold{69} & \colcellnobold{63} & \colcellnobold{93} & \colcellnobold{56} & \colcellnobold{75} & \colcellnobold{98} & \colcellnobold{88} & \colcellnobold{81} & \colcellnobold{76} & \colcellnobold{94} & \colcellnobold{94} & \colcellnobold{95} & \colcellnobold{96} & \colcellnobold{76} \\
    \textbf{\agentName} & \colcellnobold{62} & \colcellnobold{59} & -  & \colcellnobold{49} & \colcellnobold{64} & \colcellnobold{71} & \colcellnobold{64} & \colcellnobold{64} & \colcellnobold{64} & \colcellnobold{78} & \colcellnobold{78} & \colcellnobold{76} & \colcellnobold{84} & \colcellnobold{94} & \colcellnobold{73} & \colcellnobold{87} & \colcellnobold{87} & \colcellnobold{72} \\
    ObiBotKenobi   & \colcellnobold{39} & \colcellnobold{29} & \colcellnobold{47} & -  & \colcellnobold{47} & \colcellnobold{69} & \colcellnobold{60} & \colcellnobold{56} & \colcellnobold{58} & \colcellnobold{83} & \colcellnobold{65} & \colcellnobold{76} & \colcellnobold{72} & \colcellnobold{99} & \colcellnobold{79} & \colcellnobold{85} & \colcellnobold{100} & \colcellnobold{66} \\
    Aggrobot       & \colcellnobold{9}  & \colcellnobold{25} & \colcellnobold{26} & \colcellnobold{60} & -  & \colcellnobold{69} & \colcellnobold{55} & \colcellnobold{44} & \colcellnobold{63} & \colcellnobold{86} & \colcellnobold{69} & \colcellnobold{94} & \colcellnobold{66} & \colcellnobold{94} & \colcellnobold{94} & \colcellnobold{91} & \colcellnobold{94} & \colcellnobold{65} \\
    sophia         & \colcellnobold{25} & \colcellnobold{44} & \colcellnobold{30} & \colcellnobold{35} & \colcellnobold{38} & - & \colcellnobold{41} & \colcellnobold{88} & \colcellnobold{75} & \colcellnobold{76} & \colcellnobold{63} & \colcellnobold{69} & \colcellnobold{71} & \colcellnobold{100} & \colcellnobold{75} & \colcellnobold{84} & \colcellnobold{83} & \colcellnobold{62} \\
    bRHEAdBot      & \colcellnobold{4}  & \colcellnobold{7}  & \colcellnobold{24} & \colcellnobold{44} & \colcellnobold{49} & \colcellnobold{69} & -  & \colcellnobold{51} & \colcellnobold{64} & \colcellnobold{79} & \colcellnobold{59} & \colcellnobold{65} & \colcellnobold{83} & \colcellnobold{99} & \colcellnobold{81} & \colcellnobold{96} & \colcellnobold{98} & \colcellnobold{61} \\
    Ragnar         & \colcellnobold{40} & \colcellnobold{50} & \colcellnobold{32} & \colcellnobold{26} & \colcellnobold{50} & \colcellnobold{13} & \colcellnobold{46} & -  & \colcellnobold{44} & \colcellnobold{71} & \colcellnobold{63} & \colcellnobold{69} & \colcellnobold{73} & \colcellnobold{88} & \colcellnobold{81} & \colcellnobold{73} & \colcellnobold{85} & \colcellnobold{56} \\
    POLightRush    & \colcellnobold{0}  & \colcellnobold{25} & \colcellnobold{29} & \colcellnobold{38} & \colcellnobold{31} & \colcellnobold{44} & \colcellnobold{34} & \colcellnobold{38} & -  & \colcellnobold{71} & \colcellnobold{69} & \colcellnobold{69} & \colcellnobold{73} & \colcellnobold{100} & \colcellnobold{75} & \colcellnobold{91} & \colcellnobold{100} & \colcellnobold{55} \\
    SaveTheBeesV4  & \colcellnobold{14} & \colcellnobold{9}  & \colcellnobold{21} & \colcellnobold{43} & \colcellnobold{31} & \colcellnobold{59} & \colcellnobold{38} & \colcellnobold{47} & \colcellnobold{66} & -  & \colcellnobold{50} & \colcellnobold{57} & \colcellnobold{81} & \colcellnobold{86} & \colcellnobold{85} & \colcellnobold{90} & \colcellnobold{93} & \colcellnobold{54} \\
    POWorkerRush   & \colcellnobold{13} & \colcellnobold{13} & \colcellnobold{21} & \colcellnobold{29} & \colcellnobold{31} & \colcellnobold{44} & \colcellnobold{44} & \colcellnobold{56} & \colcellnobold{38} & \colcellnobold{89} & - & \colcellnobold{75} & \colcellnobold{49} & \colcellnobold{94} & \colcellnobold{81} & \colcellnobold{81} & \colcellnobold{96} & \colcellnobold{53} \\
    MyMicroRtsBot  & \colcellnobold{11} & \colcellnobold{13} & \colcellnobold{15} & \colcellnobold{25} & \colcellnobold{38} & \colcellnobold{56} & \colcellnobold{38} & \colcellnobold{56} & \colcellnobold{38} & \colcellnobold{86} & \colcellnobold{44} & - & \colcellnobold{43} & \colcellnobold{94} & \colcellnobold{69} & \colcellnobold{74} & \colcellnobold{92} & \colcellnobold{49} \\
    NaiveMCTS      & \colcellnobold{0}  & \colcellnobold{11} & \colcellnobold{17} & \colcellnobold{22} & \colcellnobold{34} & \colcellnobold{27} & \colcellnobold{15} & \colcellnobold{26} & \colcellnobold{29} & \colcellnobold{69} & \colcellnobold{56} & \colcellnobold{58} & - & \colcellnobold{92} & \colcellnobold{46} & \colcellnobold{60} & \colcellnobold{84} & \colcellnobold{40} \\
    myBot          & \colcellnobold{1}  & \colcellnobold{6}  & \colcellnobold{21} & \colcellnobold{20} & \colcellnobold{39} & \colcellnobold{48} & \colcellnobold{28} & \colcellnobold{41} & \colcellnobold{43} & \colcellnobold{77} & \colcellnobold{39} & \colcellnobold{40} & \colcellnobold{50} & - & \colcellnobold{55} & \colcellnobold{66} & \colcellnobold{66} & \colcellnobold{40} \\
    NIlSiBot       & \colcellnobold{0}  & \colcellnobold{13} & \colcellnobold{18} & \colcellnobold{18} & \colcellnobold{31} & \colcellnobold{25} & \colcellnobold{13} & \colcellnobold{13} & \colcellnobold{31} & \colcellnobold{63} & \colcellnobold{31} & \colcellnobold{38} & \colcellnobold{51} & \colcellnobold{81} & - & \colcellnobold{58} & \colcellnobold{73} & \colcellnobold{35} \\
    Predator       & \colcellnobold{1}  & \colcellnobold{7}  & \colcellnobold{13} & \colcellnobold{6}  & \colcellnobold{12} & \colcellnobold{21} & \colcellnobold{11} & \colcellnobold{16} & \colcellnobold{14} & \colcellnobold{56} & \colcellnobold{22} & \colcellnobold{28} & \colcellnobold{44} & \colcellnobold{73} & \colcellnobold{43} & - & \colcellnobold{45} & \colcellnobold{26} \\
    RandomBiasedAI & \colcellnobold{0}  & \colcellnobold{1}  & \colcellnobold{15} & \colcellnobold{0}  & \colcellnobold{4}  & \colcellnobold{15} & \colcellnobold{6}  & \colcellnobold{9}  & \colcellnobold{4}  & \colcellnobold{52} & \colcellnobold{4}  & \colcellnobold{13} & \colcellnobold{18} & \colcellnobold{85} & \colcellnobold{39} & \colcellnobold{39} & - & \colcellnobold{19} \\
    \end{tabular}
    \end{center}
\end{supptable}

\section{Additional behavior cloning benchmarks}

\begin{supptable}[H]
    \caption{\bcAgent\ win rate in single player round-robin benchmark. Win rates above 50\% are bolded. Higher win rates are redder. Lower win rates are bluer.}
    \label{tab:bc-winrate}
    \arrayrulecolor{black}
    \begin{center}
    \begin{tabular}{lcccc|c}
     & POWorkerRush & POLightRush & CoacAI & Mayari & Overall \\
    \arrayrulecolor{black}\specialrule{.5pt}{0pt}{0pt}
    \texttt{basesWorkers8x8A} & \colcell{60} & \colcell{100} & \colcell{90} & \colcell{50} & \colcell{75} \\
    \texttt{FourBasesWorkers8x8} & \colcell{100} & \colcell{100} & \colcell{85} & \colcell{65} & \colcell{88} \\
    \texttt{NoWhereToRun9x8} & \colcell{100} & \colcell{100} & \colcell{83} & \colcell{55} & \colcell{85} \\
    \texttt{basesWorkers16x16A} & \colcell{10} & \colcell{100} & \colcell{100} & \colcell{28} & \colcell{60} \\
    \texttt{TwoBasesBarracks16x16} & \colcell{100} & \colcell{100} & \colcell{43} & \colcell{20} & \colcell{66} \\
    \texttt{DoubleGame24x24} & \colcell{0} & \colcell{100} & \colcell{100} & \colcell{30} & \colcell{58} \\
    \texttt{BWDistantResources32x32} & \colcell{48} & \colcell{100} & \colcell{100} & \colcell{65} & \colcell{78} \\
    \texttt{(4)BloodBath.scmB} & \colcell{100} & \colcell{63} & \colcell{20} & \colcell{40} & \colcell{56} \\
    \arrayrulecolor{black}\specialrule{.5pt}{0pt}{0pt}
    AI Average & \colcell{65} & \colcell{96} & \colcell{78} & \colcell{44} & \colcell{71} \\
    \end{tabular}
    \end{center}
\end{supptable}

\section{Videos of \agentName\ Against Mayari (2021 CoG Winner)}
In the \texttt{vsMayariVideos} directory of the supplemental materials, we provide videos of
\agentName\ playing against the 2021 CoG competition winner Mayari on each of the Open
maps. \agentName\ is always Player 1 in these videos, thus \agentName's units have a
blue outline while Mayari's units have a red outline. \agentName's units start at the left or
upper part of the map, except in \mapname{(4)BloodBath.scmB} where \agentName\ starts at the
bottom-right. The videos are named after the map they are played on.

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv, cog2024_supplemental}

\end{document}